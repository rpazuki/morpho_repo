{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7d92a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:31.391364Z",
     "start_time": "2023-07-12T17:39:22.232524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MEMORY = 16*1024\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.set_logical_device_configuration(gpus[0],\n",
    "                                              [tf.config.LogicalDeviceConfiguration(memory_limit=MEMORY)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:        \n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e) \n",
    "    \n",
    "import pathlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "sys.path.append(f\"{Path.home()}/morpho_repo\")\n",
    "sys.path.append(f\"{Path.home()}/morpho_repo/turing_codebase\")\n",
    "from turing.utils import *\n",
    "from turing.tf_utils import *\n",
    "import turing.pinns as tu\n",
    "from turing.loss_functions import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9846c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:32.547654Z",
     "start_time": "2023-07-12T17:39:32.530839Z"
    }
   },
   "outputs": [],
   "source": [
    "from pde_solvers.cn import *\n",
    "from local_utils import *\n",
    "from turing.three_nodes_circuits import create_circuit_3954"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a991c",
   "metadata": {},
   "source": [
    "# Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a7201f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:34.053131Z",
     "start_time": "2023-07-12T17:39:33.962326Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../df_network_analysis.csv\")\n",
    "df[\"adj_tup\"] = df[\"adj_tup\"].apply(lambda x: eval(f\"tuple({x})\"))\n",
    "df[\"Adj\"] = df[\"adj_tup\"].apply(lambda x: np.array(x).reshape((3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31caa7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:34.644737Z",
     "start_time": "2023-07-12T17:39:34.629925Z"
    }
   },
   "outputs": [],
   "source": [
    "adj=np.array([[1, 1, -1], [-1, 0, -1], [0, -1, 1]])\n",
    "subnet_list = [g[1] for g in df.groupby(\"adj_tup\") if g[0] == tuple(adj.flatten())]\n",
    "if len(subnet_list) == 0:\n",
    "    print(\"================================\")\n",
    "    print(\"There is no adjacancy matrix as: \", adj)\n",
    "    print(\"================================\")\n",
    "else:\n",
    "    subnet_df = subnet_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050355fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:35.327835Z",
     "start_time": "2023-07-12T17:39:35.294427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_A</th>\n",
       "      <th>min_B</th>\n",
       "      <th>min_C</th>\n",
       "      <th>avg_A</th>\n",
       "      <th>avg_B</th>\n",
       "      <th>avg_C</th>\n",
       "      <th>max_A</th>\n",
       "      <th>max_B</th>\n",
       "      <th>max_C</th>\n",
       "      <th>std_A</th>\n",
       "      <th>...</th>\n",
       "      <th>lb_CB</th>\n",
       "      <th>ub_CB</th>\n",
       "      <th>state_CC</th>\n",
       "      <th>lb_CC</th>\n",
       "      <th>ub_CC</th>\n",
       "      <th>Adj</th>\n",
       "      <th>k_max</th>\n",
       "      <th>params</th>\n",
       "      <th>path</th>\n",
       "      <th>adj_tup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.879311</td>\n",
       "      <td>27.4671</td>\n",
       "      <td>2.00005</td>\n",
       "      <td>3.859942</td>\n",
       "      <td>31.707845</td>\n",
       "      <td>2.000238</td>\n",
       "      <td>6.759598</td>\n",
       "      <td>35.970982</td>\n",
       "      <td>2.000543</td>\n",
       "      <td>1.49011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.780018</td>\n",
       "      <td>56.210562</td>\n",
       "      <td>Active</td>\n",
       "      <td>0.880009</td>\n",
       "      <td>28.110281</td>\n",
       "      <td>[[1, 1, -1], [-1, 0, -1], [0, -1, 1]]</td>\n",
       "      <td>8.430177</td>\n",
       "      <td>[4.0e+00 1.0e-03 1.0e-01 5.0e+00 5.0e+00 5.0e+...</td>\n",
       "      <td>outputs_second_search/solution_10_0_24.npy</td>\n",
       "      <td>(1, 1, -1, -1, 0, -1, 0, -1, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        min_A    min_B    min_C     avg_A      avg_B     avg_C     max_A  \\\n",
       "116  0.879311  27.4671  2.00005  3.859942  31.707845  2.000238  6.759598   \n",
       "\n",
       "         max_B     max_C    std_A  ...     lb_CB      ub_CB state_CC  \\\n",
       "116  35.970982  2.000543  1.49011  ...  1.780018  56.210562   Active   \n",
       "\n",
       "        lb_CC      ub_CC                                    Adj     k_max  \\\n",
       "116  0.880009  28.110281  [[1, 1, -1], [-1, 0, -1], [0, -1, 1]]  8.430177   \n",
       "\n",
       "                                                params  \\\n",
       "116  [4.0e+00 1.0e-03 1.0e-01 5.0e+00 5.0e+00 5.0e+...   \n",
       "\n",
       "                                           path  \\\n",
       "116  outputs_second_search/solution_10_0_24.npy   \n",
       "\n",
       "                             adj_tup  \n",
       "116  (1, 1, -1, -1, 0, -1, 0, -1, 1)  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subnet_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e2149f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:35.974897Z",
     "start_time": "2023-07-12T17:39:35.962622Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(f\"../{path}\", \"rb\") as f:\n",
    "        k_max, params, res = np.load(f, allow_pickle=True)\n",
    "    (n_val, \n",
    "     b_A_val, mu_A_val, V_A_val, K_AA_val, K_AB_val, K_AC_val,\n",
    "     b_B_val, mu_B_val, V_B_val, K_BA_val, K_BC_val,\n",
    "     b_C_val, mu_C_val, V_C_val, K_CB_val, K_CC_val) = params\n",
    "    params = {\n",
    "              'D_A':0.01,\n",
    "              'D_B':1.0,\n",
    "              'n':n_val, \n",
    "              'b_A':b_A_val, \n",
    "              'mu_A':mu_A_val, \n",
    "              'V_A':V_A_val,\n",
    "              'K_AA':K_AA_val, \n",
    "              'K_AB':K_AB_val,  \n",
    "              'K_AC':K_AC_val,\n",
    "              'b_B':b_B_val, \n",
    "              'mu_B':mu_B_val, \n",
    "              'V_B':V_B_val,\n",
    "              'K_BA':K_BA_val, \n",
    "              'K_BC':K_BC_val,  \n",
    "              'b_C':b_C_val, \n",
    "              'mu_C':mu_C_val, \n",
    "              'V_C':V_C_val,\n",
    "              'K_CB':K_CB_val, \n",
    "              'K_CC':K_CC_val\n",
    "             }\n",
    "           \n",
    "    return (params, res, k_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde5f018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:36.711586Z",
     "start_time": "2023-07-12T17:39:36.695182Z"
    },
    "code_folding": [
     2,
     7,
     49
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def cos_dist(arr1, arr2):\n",
    "    arr1_L = np.sqrt(np.dot(arr1, arr1))\n",
    "    arr2_L = np.sqrt(np.dot(arr2, arr2))\n",
    "    return np.dot(arr1, arr2)/(arr1_L*arr2_L)\n",
    "\n",
    "def alienor_components2(epsilon, l1, bounds):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "          epsilon (float): The accuracy of the estimates.\n",
    "          l1 (float): Lipschitz constant\n",
    "          bounds (list of tuples):\n",
    "          bounds like [a_i, b_i], for each variables separatly.\n",
    "          \n",
    "          \n",
    "    \"\"\"\n",
    "    n = len(bounds) \n",
    "    assert n >= 2, \"The method expects two or more vairbales.\"\n",
    "    assert np.all([len(item) == 2 \n",
    "                   for item in bounds]), \"bounds must be tuple of (a_i,b_i)\"\n",
    "    \n",
    "    alpha = epsilon/(2*l1*np.sqrt(n-1))\n",
    "    alphas = np.ones(n)\n",
    "    \n",
    "    def get_h_i(a,b,alpha):\n",
    "        def h_i(t):\n",
    "            return (a-b)*np.cos(alpha*t)/2 + (a+b)/2\n",
    "        return h_i\n",
    "    a,b = bounds[0]\n",
    "    h_i = get_h_i(a,b,1)\n",
    "    h_list = [h_i]\n",
    "    for i in range(1, n):\n",
    "        # [a_i, b_i]\n",
    "        a,b = bounds[i]\n",
    "        # alphas[i-1] (alpha/pi) / (|b_i| + |a_i|)\n",
    "        alphas[i] = alpha*alphas[i-1]/(np.pi*(np.abs(b)+np.abs(a)))\n",
    "        # h_i = (a_i - b_i)cos(alpha_i theta)/2 +  (a_i + b_i)/2         \n",
    "        h_i = h_i = get_h_i(a,b,alphas[i])#\n",
    "        h_list.append(h_i)        \n",
    "        \n",
    "        \n",
    "    # l2 or Lipschitz constant of the aliemor h functions\n",
    "    l2 = np.linalg.norm([(np.abs(d[1])+np.abs(d[0]))**2 * a**2 for d,a in zip(bounds,alphas) ])/2\n",
    "    #\n",
    "    theta_max = np.pi/alphas[-1]    \n",
    "    return alpha, alphas, l2, h_list, theta_max\n",
    "\n",
    "def minim_2(epsilon, l1, bounds, func, maxiter=10000):\n",
    "    alpha, alphas, l2, h_list, theta_max = alienor_components2(epsilon, l1, bounds)\n",
    "    k = 1\n",
    "    L = l1 * l2\n",
    "    theta = epsilon / L    \n",
    "    theta_epsilon = theta\n",
    "    \n",
    "    f = lambda t: func(*[ h(t) for h in h_list])\n",
    "    f_epsilon = f_theta = f(theta_epsilon)\n",
    "    \n",
    "    while k < maxiter:\n",
    "        if theta > np.pi/alphas[-1]:            \n",
    "            return k, theta,theta_epsilon, f_epsilon, \"\"\n",
    "        \n",
    "        theta = theta + (epsilon + f_theta - f_epsilon)/ L        \n",
    "        f_theta  =  f(theta)\n",
    "        if f_theta < f_epsilon:\n",
    "            f_epsilon = f_theta\n",
    "            theta_epsilon = theta\n",
    "        k += 1    \n",
    "    return k, theta,theta_epsilon, f_epsilon, f\"max iteration '{maxiter}' is reached\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e50a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:39:37.571407Z",
     "start_time": "2023-07-12T17:39:37.553350Z"
    },
    "code_folding": [
     3,
     45
    ]
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, shared_memory\n",
    "\n",
    "@tf.function\n",
    "def grads(pinn, H):\n",
    "    def flatten(arr):\n",
    "        return tf.reshape(arr, (arr.shape[0]*arr.shape[1]*arr.shape[2], arr.shape[3]))\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:        \n",
    "        H = flatten(H)\n",
    "        tape.watch(H)\n",
    "        \n",
    "        outputs = pinn.net(H)\n",
    "        Ag = tf.squeeze(outputs[:, 0])\n",
    "        Bg = tf.squeeze(outputs[:, 1])\n",
    "        Cg = tf.squeeze(outputs[:, 2])\n",
    "        \n",
    "\n",
    "        grad_A = tape.gradient(Ag, H)\n",
    "        A_x = grad_A[:, 0]\n",
    "        A_y = grad_A[:, 1]\n",
    "        A_t = grad_A[:, 2]\n",
    "\n",
    "        grad_B = tape.gradient(Bg, H)\n",
    "        B_x = grad_B[:, 0]\n",
    "        B_y = grad_B[:, 1]\n",
    "        B_t = grad_B[:, 2]\n",
    "\n",
    "\n",
    "        grad_A_x = tape.gradient(A_x, H)\n",
    "        A_xx = grad_A_x[:, 0]\n",
    "        grad_A_y = tape.gradient(A_y, H)\n",
    "        A_yy = grad_A_y[:, 1]\n",
    "        grad_B_x = tape.gradient(B_x, H)\n",
    "        B_xx = grad_B_x[:, 0]\n",
    "        grad_B_y = tape.gradient(B_y, H)\n",
    "        B_yy = grad_B_y[:, 1]\n",
    "        \n",
    "        \n",
    "    return (tf.squeeze(Ag), tf.squeeze(A_xx), tf.squeeze(A_yy), tf.squeeze(A_t),\n",
    "            tf.squeeze(Bg), tf.squeeze(B_xx), tf.squeeze(B_yy), tf.squeeze(B_t),\n",
    "            tf.squeeze(Cg)\n",
    "           )                               \n",
    "\n",
    "class _LocalFunctions:\n",
    "    @classmethod\n",
    "    def add_functions(cls, *args):\n",
    "        for function in args:\n",
    "            setattr(cls, function.__name__, function)\n",
    "            function.__qualname__ = cls.__qualname__ + '.' + function.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e61c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:40:26.006379Z",
     "start_time": "2023-07-12T17:39:39.062081Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "D_A_val, D_B_val = 0.01, 1.0\n",
    "N=5000\n",
    "T=100\n",
    "delta_t = T/N\n",
    "model_128_10 = RD_2D_1st_Order(Ds=[D_A_val, D_B_val, 0], \n",
    "                               delta_t=delta_t, Lx=10, Ly=10, \n",
    "                               Ix=128, Jy=128,\n",
    "                               boundary_condition=Neumann_Boundary_2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa4edbdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:40:32.841971Z",
     "start_time": "2023-07-12T17:40:32.785796Z"
    }
   },
   "outputs": [],
   "source": [
    "pool_num = 28\n",
    "models_list = np.array([ copy.deepcopy(model_128_10) for _ in range(pool_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a2c588d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:40:35.438948Z",
     "start_time": "2023-07-12T17:40:35.004451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta theta A: 2.222222222222222e-06\n",
      "delta theta A: 450000.0\n",
      "\n",
      "delta theta B: 2.222222222222222e-06\n",
      "delta theta B: 450000.0\n",
      "\n",
      "delta theta C: 2.222222222222222e-06\n",
      "delta theta C: 450000.0\n",
      "A epochs: 45.0\n",
      "B epochs: 45.0\n",
      "C epochs: 45.0\n"
     ]
    }
   ],
   "source": [
    "bounds_A = [(0,30), (0,200), (0,30), (0,30)]\n",
    "bounds_B = [(0,30), (0,200), (0,30), (0,30)]\n",
    "bounds_C = [(0,30), (0,200), (0,30), (0,30), (0,30)]\n",
    "epsilon = 1e-3\n",
    "l1 = 1\n",
    "\n",
    "alpha_A, alphas_A, l2_A, h_list2_A, theta_max2_A = alienor_components2(epsilon, l1, bounds_A)\n",
    "alpha_B, alphas_B, l2_B, h_list2_B, theta_max2_B = alienor_components2(epsilon, l1, bounds_B)\n",
    "alpha_C, alphas_C, l2_C, h_list2_C, theta_max2_C = alienor_components2(epsilon, l1, bounds_C)\n",
    "L_A = l1*l2_A\n",
    "L_B = l1*l2_B\n",
    "L_C = l1*l2_C\n",
    "print(\"delta theta A:\", epsilon/L_A)\n",
    "print(\"delta theta A:\", L_A/epsilon)\n",
    "print()\n",
    "print(\"delta theta B:\", epsilon/L_B)\n",
    "print(\"delta theta B:\", L_B/epsilon)\n",
    "print()\n",
    "print(\"delta theta C:\", epsilon/L_C)\n",
    "print(\"delta theta C:\", L_C/epsilon)\n",
    "\n",
    "batch_size = 10000\n",
    "print(\"A epochs:\", L_A/epsilon/batch_size)\n",
    "print(\"B epochs:\", L_B/epsilon/batch_size)\n",
    "print(\"C epochs:\", L_C/epsilon/batch_size)\n",
    "\n",
    "thetas_A = np.linspace(0, theta_max2_A,  int(L_A/epsilon))\n",
    "thetas_B = np.linspace(0, theta_max2_B,  int(L_B/epsilon))\n",
    "thetas_C = np.linspace(0, theta_max2_C,  int(L_C/epsilon))\n",
    "\n",
    "params_by_theta_A = np.stack([h_list2_A[0](thetas_A), h_list2_A[1](thetas_A),\n",
    "                              h_list2_A[2](thetas_A), h_list2_A[3](thetas_A)]).T\n",
    "\n",
    "params_by_theta_B = np.stack([h_list2_B[0](thetas_B), h_list2_B[1](thetas_B),\n",
    "                              h_list2_B[2](thetas_B), h_list2_B[3](thetas_B)]).T\n",
    "\n",
    "params_by_theta_C = np.stack([h_list2_C[0](thetas_C), h_list2_C[1](thetas_C),\n",
    "                              h_list2_C[2](thetas_C), h_list2_C[3](thetas_C),\n",
    "                              h_list2_C[4](thetas_C)]).T\n",
    "\n",
    "theta_A_n = params_by_theta_A.shape[0]\n",
    "theta_A_m = params_by_theta_A.shape[1]\n",
    "theta_B_n = params_by_theta_B.shape[0]\n",
    "theta_B_m = params_by_theta_B.shape[1]\n",
    "theta_C_n = params_by_theta_C.shape[0]\n",
    "theta_C_m = params_by_theta_C.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34611d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:40:37.675425Z",
     "start_time": "2023-07-12T17:40:37.665839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.443603515625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10 - 10/128)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd597c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:40:39.225311Z",
     "start_time": "2023-07-12T17:40:39.158431Z"
    },
    "code_folding": [
     15,
     19,
     26,
     30,
     72,
     95,
     118,
     141,
     157,
     210,
     270,
     279,
     324
    ]
   },
   "outputs": [],
   "source": [
    "def to(arr):\n",
    "    return arr.reshape(128, 128) \n",
    "\n",
    "def reshape(arr, steps=1):\n",
    "    T = arr.shape[0]\n",
    "    ret = np.array([\n",
    "        [to(arr[i, 0, :]), to(arr[i, 1, :]), to(arr[i, 2, :])]\n",
    "        for i in range(T-steps, T)\n",
    "    ])\n",
    "    return np.einsum(\"tcxy -> cxyt\", ret)\n",
    "\n",
    "def rmse(arr1, arr2):\n",
    "    return np.sqrt(np.mean((arr1-arr2)**2))\n",
    "\n",
    "\n",
    "def G(sigma, x, y):\n",
    "    gaussian = (1/(2*np.pi*sigma**2))*np.exp(-(x**2+y**2)/(2*sigma**2))\n",
    "    return gaussian\n",
    "\n",
    "def G_discrete(sigma, n):\n",
    "    l = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            l[i,j] = G(sigma, (i-(n-1)/2),(j-(n-1)/2))\n",
    "    return l\n",
    "\n",
    "def G_discrete_normalise(sigma, n):\n",
    "    l = G_discrete(sigma, n)\n",
    "    return l/np.sum(l)\n",
    "\n",
    "def search_for_params(index, \n",
    "                      pinn,\n",
    "                      A_max, A_min,\n",
    "                      B_max, B_min,\n",
    "                      C_max, C_min,\n",
    "                      n_max = 20,\n",
    "                      slice_min = 45, \n",
    "                      slice_max = 100,):\n",
    "    print(\"=\"*40)\n",
    "    path = subnet_df[\"path\"].iloc[index]\n",
    "    print(index, path)\n",
    "    (params, res_1, k_max) = load_dataset(path)\n",
    "\n",
    "    n_val = params[\"n\"]\n",
    "    mu_A_val, mu_B_val, mu_C_val = params[\"mu_A\"], params[\"mu_B\"], params[\"mu_C\"]\n",
    "    D_A = params[\"D_A\"]\n",
    "    D_B = params[\"D_B\"]\n",
    "\n",
    "    def Create_f_A(A, B, C, n, mu_A, A_diffu=None):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]           \n",
    "        #A_diffu =  0.01* diffu_2D(A).flatten()[np.newaxis, :]\n",
    "        if A_diffu is None:\n",
    "            dxdy = (10*10)/((A.shape[0]-1)*(A.shape[1]-1))\n",
    "            A_diffu = ((1.0/dxdy)*(diffusion((A.shape[0],A.shape[1]),A))).flatten()[np.newaxis, :]\n",
    "\n",
    "        def L_2_f_a(args):\n",
    "            (b_A, V_A,  K_AA, K_BA\n",
    "            ) = (args[:, 0:1], args[:, 1:2], args[:, 2:3], args[:, 3:4])\n",
    "            f1 = (b_A + V_A*act(A_flat, K_AA, n)*inh(B_flat, K_BA, n) - mu_A*A_flat \n",
    "                  + D_A*A_diffu)\n",
    "            f2 = (b_A/(D_A+1e-6) + V_A*act(A_flat, K_AA, n)*inh(B_flat, K_BA, n)/(D_A+1e-6) \n",
    "                  - mu_A*A_flat/(D_A+1e-6) + A_diffu)\n",
    "\n",
    "            return np.sum((f1**2 + f2**2 )/A_flat.size, axis=1)\n",
    "        return L_2_f_a\n",
    "\n",
    "    def Create_f_A_1_D(A, B, C, n, mu_A, A_diffu=None):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]    \n",
    "        #A_diffu =  0.01* diffu_2D(A).flatten()[np.newaxis, :]\n",
    "        if A_diffu is None:\n",
    "            dxdy = (10*10)/((A.shape[0]-1)*(A.shape[1]-1))\n",
    "            A_diffu = ((1.0/dxdy)*(diffusion((A.shape[0],A.shape[1]),A))).flatten()[np.newaxis, :]\n",
    "        def L_2_f_a(args):        \n",
    "            (b_A,V_A, K_AA, K_BA\n",
    "            ) = (args[0], args[1], args[2], args[3])\n",
    "            f1 = (b_A + V_A*act(A_flat, K_AA, n)*inh(B_flat, K_BA, n) - mu_A*A_flat \n",
    "                  + D_A*A_diffu)\n",
    "            f2 = (b_A/(D_A+1e-6) + V_A*act(A_flat, K_AA, n)*inh(B_flat, K_BA, n)/(D_A+1e-6) \n",
    "                  - mu_A*A_flat/(D_A+1e-6) + A_diffu)\n",
    "\n",
    "            return np.sum((f1**2 + f2**2 )/A_flat.size, axis=1)\n",
    "        return L_2_f_a\n",
    "\n",
    "    def Create_f_B(A, B, C, n, mu_B, B_diffu=None):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]   \n",
    "        C_flat = C#.flatten()[np.newaxis, :]\n",
    "        #B_diffu =  1.0* diffu_2D(B).flatten()[np.newaxis, :]\n",
    "        if B_diffu is None:\n",
    "            dxdy = (10*10)/((B.shape[0]-1)*(B.shape[1]-1))\n",
    "            B_diffu = ((1.0/dxdy)*(diffusion((B.shape[0],B.shape[1]),B))).flatten()[np.newaxis, :]    \n",
    "        def L_2_f_b(args):\n",
    "            (b_B, V_B, K_AB, K_CB\n",
    "            ) = (args[:, 0:1], args[:, 1:2], args[:, 2:3], args[:, 3:4])\n",
    "            f1 = (b_B + V_B*act(A_flat, K_AB, n)*inh(C_flat, K_CB, n) - mu_B*B_flat \n",
    "                 + D_B*B_diffu)\n",
    "            f2 = (b_B/(D_B + 1e-6) + V_B*act(A_flat, K_AB, n)*inh(C_flat, K_CB, n)/(D_B + 1e-6)\n",
    "                  - mu_B*B_flat/(D_B + 1e-6)  + B_diffu)\n",
    "            return np.sum((f1**2 + f2**2 )/B_flat.size, axis=1)\n",
    "        return L_2_f_b\n",
    "\n",
    "    def Create_f_B_1_D(A, B, C, n, mu_B, B_diffu=None):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]   \n",
    "        C_flat = C#.flatten()[np.newaxis, :]\n",
    "        #B_diffu =  1.0* diffu_2D(B).flatten()[np.newaxis, :]\n",
    "        if B_diffu is None:\n",
    "            dxdy = (10*10)/((B.shape[0]-1)*(B.shape[1]-1))\n",
    "            B_diffu = ((1.0/dxdy)*(diffusion((B.shape[0],B.shape[1]),B))).flatten()[np.newaxis, :]\n",
    "        def L_2_f_b(args):        \n",
    "            (b_B, V_B,K_AB, K_CB\n",
    "            )  = (args[0], args[1], args[2], args[3])\n",
    "            f1 = (b_B + V_B*act(A_flat, K_AB, n)*inh(C_flat, K_CB, n) - mu_B*B_flat \n",
    "                 + D_B*B_diffu)\n",
    "            f2 = (b_B/(D_B + 1e-6) + V_B*act(A_flat, K_AB, n)*inh(C_flat, K_CB, n)/(D_B + 1e-6)\n",
    "                  - mu_B*B_flat/(D_B + 1e-6)  + B_diffu)\n",
    "            return np.sum((f1**2 + f2**2 )/B_flat.size, axis=1)\n",
    "        return L_2_f_b\n",
    "\n",
    "    def Create_f_C(A, B, C, n, mu_C):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]   \n",
    "        C_flat = C#.flatten()[np.newaxis, :]\n",
    "        def L_2_f_c(args):\n",
    "            b_C, V_C, K_AC, K_BC, K_CC = args[:, 0:1], args[:, 1:2], args[:, 2:3], args[:, 3:4], args[:, 4:5]\n",
    "            #b_C, V_C, K_AC, K_BC, K_CC = args[0], args[1], args[2], args[3], args[4]\n",
    "            f = b_C + V_C*inh(A_flat, K_AC, n)*inh(B_flat, K_BC, n)*act(C_flat, K_CC, n) - mu_C * C_flat\n",
    "            return np.sum(f**2, axis=1)\n",
    "        return L_2_f_c\n",
    "\n",
    "    def Create_f_C_1_D(A, B, C, n, mu_C):\n",
    "        def act(x, km, n=4):\n",
    "            return 1 / (1 + (km / (x + 1e-20)) ** (n))\n",
    "\n",
    "        def inh(x, km, n=4):\n",
    "            return 1 / (1 + (x / (km + 1e-20)) ** (n))\n",
    "        A_flat = A#.flatten()[np.newaxis, :]\n",
    "        B_flat = B#.flatten()[np.newaxis, :]   \n",
    "        C_flat = C#.flatten()[np.newaxis, :]\n",
    "        def L_2_f_c(args):        \n",
    "            b_C, V_C, K_AC, K_BC, K_CC = args[0], args[1], args[2], args[3], args[4]\n",
    "            f = b_C + V_C*inh(A_flat, K_AC, n)*inh(B_flat, K_BC, n)*act(C_flat, K_CC, n) - mu_C * C_flat\n",
    "            return np.sum(f**2)\n",
    "        return L_2_f_c\n",
    "    ####################################################\n",
    "    def flatten(arr):\n",
    "        return tf.reshape(arr, (arr.shape[0]*arr.shape[1]*arr.shape[2], arr.shape[3]))   \n",
    "    ####################################################\n",
    "    T=1    \n",
    "    L=1\n",
    "    data = reshape(res_1, T)\n",
    "    nodes_n = data.shape[0]\n",
    "    node_names = [\"A\", \"B\", \"C\"]\n",
    "    x_size = data.shape[1]\n",
    "    y_size = data.shape[2]\n",
    "    ##########################################\n",
    "    # Create a mesh that is the centers of the\n",
    "    # original mesh\n",
    "    x_size -= 1\n",
    "    y_size -= 1\n",
    "    dxdy = (10 - 10/127)**2/((x_size-1)*(y_size-1))\n",
    "    ##########################\n",
    "    N = x_size*y_size    \n",
    "    t_star = np.arange(T, T+1)\n",
    "    ##########################\n",
    "    x_slice = slice(slice_min, slice_max, 1)\n",
    "    y_slice = slice(slice_min, slice_max, 1)\n",
    "\n",
    "    x_range = L * np.linspace(0, x_size, x_size+1)[x_slice]\n",
    "    y_range = L * np.linspace(0, y_size, y_size+1)[y_slice]\n",
    "\n",
    "    block_x = x_range.shape[0]\n",
    "    block_y = y_range.shape[0]\n",
    "\n",
    "    x = tf.constant(x_range, dtype=tf.float32)\n",
    "    y = tf.constant(y_range, dtype=tf.float32)\n",
    "    # The order of the Y and X must be reversed,\n",
    "    # since the chnages the value finds the derivatives\n",
    "    #Y, X = tf.meshgrid(x, y)\n",
    "\n",
    "    X, Y = tf.meshgrid(x, y)\n",
    "    ts = tf.constant(t_star, dtype=tf.float32)\n",
    "    T = ts[tf.newaxis, tf.newaxis, :] * tf.ones(X.shape)[:, :, tf.newaxis]\n",
    "    def H_cube(X, Y, T):\n",
    "        return tf.concat(\n",
    "                [\n",
    "                    tf.concat(\n",
    "                        [\n",
    "                            X[tf.newaxis, :, :, tf.newaxis],\n",
    "                            Y[tf.newaxis, :, :, tf.newaxis],\n",
    "                            T[:, :, i : i + 1][tf.newaxis, :, :, :],\n",
    "                        ],\n",
    "                        axis=3,\n",
    "                    )\n",
    "                    for i in range(T.shape[-1])\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "    H = H_cube(X, Y, T) \n",
    "    ##########################################\n",
    "    def to(arr):\n",
    "        return arr.numpy().reshape(block_x, block_y)\n",
    "    \n",
    "    (A, A_xx, A_yy, A_t,\n",
    "     B, B_xx, B_yy, B_t,\n",
    "     C, \n",
    "    ) = grads(pinn, H)\n",
    "    # Rmeomve the boundary effects due to convolutions, etc.\n",
    "    sub_slice = slice(2,-2,1)\n",
    "    A = to(A).copy()[sub_slice, sub_slice]\n",
    "    B = to(B).copy()[sub_slice, sub_slice]\n",
    "    C = to(C).copy()[sub_slice, sub_slice]\n",
    "    A_diff = to((A_xx + A_yy))[sub_slice, sub_slice]\n",
    "    B_diff = to((B_xx + B_yy))[sub_slice, sub_slice]\n",
    "    # Transform back to the original space\n",
    "    A = (((A+1)*(A_max-A_min))/2) + A_min\n",
    "    B = (((B+1)*(B_max-B_min))/2) + B_min\n",
    "    C = (((C+1)*(C_max-C_min))/2) + C_min\n",
    "    A_diff = A_diff*(A_max-A_min)/2\n",
    "    B_diff = B_diff*(B_max-B_min)/2\n",
    "    # Flatten the arrays\n",
    "    A = A.flatten()[np.newaxis, :]\n",
    "    B = B.flatten()[np.newaxis, :]\n",
    "    C = C.flatten()[np.newaxis, :]\n",
    "    A_diff = A_diff.flatten()[np.newaxis, :]/dxdy\n",
    "    B_diff = B_diff.flatten()[np.newaxis, :]/dxdy\n",
    "    #####################################################\n",
    "    f_a_loss = Create_f_A(A, B, C, 4, params[\"mu_A\"], A_diff)\n",
    "    f_a_loss_1_D = Create_f_A_1_D(A, B, C, 4, params[\"mu_A\"], A_diff)\n",
    "    f_b_loss = Create_f_B(A, B, C, 4, params[\"mu_B\"], B_diff)\n",
    "    f_b_loss_1_D = Create_f_B_1_D(A, B, C, 4, params[\"mu_B\"], B_diff)\n",
    "    f_c_loss = Create_f_C(A, B, C, 4, params[\"mu_C\"])\n",
    "    f_c_loss_1_D = Create_f_C_1_D(A, B, C, 4, params[\"mu_C\"])\n",
    "    #######################################################    \n",
    "    def singA(args):\n",
    "        batch_id,theta_n,theta_m= args \n",
    "        params_shm = shared_memory.SharedMemory(name=\"params_by_theta3\")\n",
    "        output_shm = shared_memory.SharedMemory(name=\"outputs3\")\n",
    "        thetas = np.ndarray((theta_n,theta_m), dtype=np.float64, buffer=params_shm.buf)\n",
    "        f_thetas = np.ndarray((theta_n), dtype=np.float64, buffer=output_shm.buf)\n",
    "        f_thetas[batch_id*batch_size:(batch_id+1)*batch_size] = f_a_loss(thetas[batch_id*batch_size:(batch_id+1)*batch_size, :])\n",
    "        return batch_id\n",
    "\n",
    "    def singB(args):\n",
    "        batch_id,theta_n,theta_m= args \n",
    "        params_shm = shared_memory.SharedMemory(name=\"params_by_theta3\")\n",
    "        output_shm = shared_memory.SharedMemory(name=\"outputs3\")\n",
    "        thetas = np.ndarray((theta_n,theta_m), dtype=np.float64, buffer=params_shm.buf)\n",
    "        f_thetas = np.ndarray((theta_n), dtype=np.float64, buffer=output_shm.buf)\n",
    "        f_thetas[batch_id*batch_size:(batch_id+1)*batch_size] = f_b_loss(thetas[batch_id*batch_size:(batch_id+1)*batch_size, :])\n",
    "        return batch_id\n",
    "\n",
    "    def singC(args):\n",
    "        batch_id,theta_n,theta_m= args \n",
    "        params_shm = shared_memory.SharedMemory(name=\"params_by_theta3\")\n",
    "        output_shm = shared_memory.SharedMemory(name=\"outputs3\")\n",
    "        thetas = np.ndarray((theta_n,theta_m), dtype=np.float64, buffer=params_shm.buf)\n",
    "        f_thetas = np.ndarray((theta_n), dtype=np.float64, buffer=output_shm.buf)\n",
    "        f_thetas[batch_id*batch_size:(batch_id+1)*batch_size] = f_c_loss(thetas[batch_id*batch_size:(batch_id+1)*batch_size, :])\n",
    "        return batch_id\n",
    "\n",
    "    _LocalFunctions.add_functions(singA, singB, singC)\n",
    "    \n",
    "    def run(theta_n,theta_m,params_by_theta, sing, L, epsilon, batch_size):\n",
    "        shm = shared_memory.SharedMemory(name=\"params_by_theta3\",\n",
    "                                         create=True, \n",
    "                                         size=params_by_theta.nbytes)\n",
    "        shared_thetas = np.ndarray((theta_n,theta_m), dtype=np.float64,\n",
    "                                    buffer=shm.buf)\n",
    "\n",
    "        shared_thetas[:,:] = params_by_theta[:,:]\n",
    "\n",
    "\n",
    "        f_thetas = np.zeros(theta_n)\n",
    "        shm_out = shared_memory.SharedMemory(name=\"outputs3\",create=True, size=f_thetas.nbytes)\n",
    "        shared_outputs = np.ndarray((theta_n), dtype=np.float64,\n",
    "                                     buffer=shm_out.buf)\n",
    "\n",
    "        \n",
    "        args =[ (batch_id, theta_n,theta_m) for batch_id in range(int(L/epsilon/batch_size) + 1)]\n",
    "\n",
    "        with Pool(55) as pool:\n",
    "            res = pool.map(sing, args)\n",
    "\n",
    "        f_thetas[:] = shared_outputs[:]\n",
    "        shm.close()\n",
    "        shm.unlink()\n",
    "\n",
    "        shm_out.close()\n",
    "        shm_out.unlink()\n",
    "\n",
    "        return f_thetas\n",
    "     \n",
    "    f_thetas_A = run(theta_A_n,theta_A_m,params_by_theta_A, singA, L_A, epsilon, batch_size)\n",
    "    f_thetas_B = run(theta_B_n,theta_B_m,params_by_theta_B, singB, L_B, epsilon, batch_size)\n",
    "    f_thetas_C = run(theta_C_n,theta_C_m,params_by_theta_C, singC, L_C, epsilon, batch_size)\n",
    "    ##########################################################\n",
    "    def minimise_top_n(n, h_list, bounds, loss_1_D, f_thetas, thetas):\n",
    "        shift = 0\n",
    "        init_params = np.zeros((n, len(h_list)))\n",
    "        final_params = np.zeros((n, len(h_list)))\n",
    "        init_loss = np.zeros(n)\n",
    "        final_loss = np.zeros(n)\n",
    "\n",
    "        top_n = np.argpartition(-f_thetas, -n)[-n:]\n",
    "        top_n = top_n[np.argsort(f_thetas[top_n])]\n",
    "\n",
    "        for shift in range(n):\n",
    "\n",
    "            theta_star = thetas[top_n][0 + shift]\n",
    "            init_par = tuple([h(theta_star) for h in h_list])\n",
    "            init_params[shift, :] = init_par\n",
    "            init_loss[shift] = f_thetas[top_n][0 + shift]\n",
    "            #bounds = ((0, 200), (0, 200), (0, 200), (0, 200), (0, 200))\n",
    "            res3= minimize(loss_1_D, x0=init_par, method='L-BFGS-B', bounds=bounds)#, options={'ftol':1e-10})\n",
    "            final_params[shift, :] = res3['x']\n",
    "            final_loss[shift] = res3['fun']\n",
    "\n",
    "        sorted_loss_ind = np.argsort(final_loss)\n",
    "        init_params = init_params[sorted_loss_ind.tolist()]\n",
    "        init_loss = init_loss[sorted_loss_ind.tolist()]\n",
    "        final_params = final_params[sorted_loss_ind.tolist()]\n",
    "        final_loss = final_loss[sorted_loss_ind.tolist()]\n",
    "\n",
    "        return (final_loss, final_params, init_loss, init_params )\n",
    "    \n",
    "    (final_loss_A, final_params_A, \n",
    "     init_loss_A, init_params_A) = minimise_top_n(n_max, h_list2_A, \n",
    "                                                  ((0, 500), (0, 500), (0, 500), (0, 500)), \n",
    "                                                  f_a_loss_1_D, f_thetas_A, thetas_A)\n",
    "\n",
    "    (final_loss_B, final_params_B, \n",
    "     init_loss_B, init_params_B) = minimise_top_n(n_max, h_list2_B, \n",
    "                                                  ((0, 500), (0, 500), (0, 500), (0, 500)), \n",
    "                                                  f_b_loss_1_D, f_thetas_B, thetas_B)\n",
    "\n",
    "    (final_loss_C, final_params_C, \n",
    "     init_loss_C, init_params_C) = minimise_top_n(n_max, h_list2_C, \n",
    "                                                  ((0, 500), (0, 500), (0, 500), (0, 500), (0, 500)), \n",
    "                                                  f_c_loss_1_D, f_thetas_C, thetas_C)\n",
    "    \n",
    "    return (path,\n",
    "            (final_loss_A, final_params_A, \n",
    "             init_loss_A, init_params_A),\n",
    "            (final_loss_B, final_params_B, \n",
    "             init_loss_B, init_params_B),\n",
    "            (final_loss_C, final_params_C, \n",
    "             init_loss_C, init_params_C)\n",
    "           )\n",
    "\n",
    "def single_simulation(args):\n",
    "    (iter_i, \n",
    "     (simulate_from_start,\n",
    "     i,\n",
    "     index, \n",
    "     run, \n",
    "     path, \n",
    "     (b_A_val, V_A_val,  K_AA_val,  K_BA_val), \n",
    "     (b_B_val, V_B_val,  K_AB_val,  K_CB_val), \n",
    "     (b_C_val, V_C_val,  K_AC_val,  K_BC_val, K_CC_val))) = args\n",
    "    \n",
    "    (params, res_1, _) = load_dataset(path)    \n",
    "    if simulate_from_start:\n",
    "        A_init = res_1[0, 0, :]\n",
    "        B_init = res_1[0, 1, :]\n",
    "        C_init = res_1[0, 2, :]\n",
    "    else:\n",
    "        A_init = res_1[-1, 0, :]\n",
    "        B_init = res_1[-1, 1, :]\n",
    "        C_init = res_1[-1, 2, :]\n",
    "    \n",
    "    n_val = params[\"n\"]\n",
    "    mu_A_val = params[\"mu_A\"]\n",
    "    mu_B_val = params[\"mu_B\"]\n",
    "    mu_C_val = params[\"mu_C\"]\n",
    "        \n",
    "    actual_params = np.array(list(params.values())[3:])\n",
    "    estimated_params = np.array(\n",
    "        [b_A_val, mu_A_val, V_A_val, K_AA_val, K_AB_val,\n",
    "         K_AC_val, b_B_val, mu_B_val, V_B_val, K_BA_val,\n",
    "         K_BC_val, b_C_val, mu_C_val, V_C_val, K_CB_val, K_CC_val])\n",
    "    euclidian_dist = np.linalg.norm(actual_params-estimated_params)\n",
    "    c_dist = cos_dist(actual_params, estimated_params)\n",
    "    kinetics = create_circuit_3954(n_val, \n",
    "                                   b_A_val, mu_A_val, V_A_val, K_AA_val, K_AB_val, K_AC_val,\n",
    "                                   b_B_val, mu_B_val, V_B_val, K_BA_val, K_BC_val,\n",
    "                                   b_C_val, mu_C_val, V_C_val, K_CB_val, K_CC_val)  \n",
    "    model_shm = shared_memory.SharedMemory(name=\"models_list\")\n",
    "    models = np.ndarray((pool_num,), \n",
    "                        dtype=models_list.dtype, \n",
    "                        buffer=model_shm.buf)\n",
    "    model_128_10 = models[iter_i%pool_num]\n",
    "\n",
    "    res_2 = model_128_10.integrate([A_init,B_init,C_init], kinetics, 5000-1, 500)\n",
    "        \n",
    "    with open(f\"./temp/res_{index}_{run}_{i}_{0 if simulate_from_start else 1}.npy\", \"wb\") as f:\n",
    "        np.save(f, res_2)\n",
    "    \n",
    "    return (simulate_from_start,\n",
    "            i,\n",
    "            index, \n",
    "            run, \n",
    "            path,\n",
    "            (n_val, \n",
    "             b_A_val, mu_A_val, V_A_val, K_AA_val, K_AB_val, K_AC_val,\n",
    "             b_B_val, mu_B_val, V_B_val, K_BA_val, K_BC_val,\n",
    "             b_C_val, mu_C_val, V_C_val, K_CB_val, K_CC_val),\n",
    "             euclidian_dist,\n",
    "             c_dist)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c40d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T17:59:28.350872Z",
     "start_time": "2023-07-12T17:40:49.127055Z"
    },
    "code_folding": [
     33
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./temp/res_69_no_noise\n",
      "A_min=0.1070347551, A_max=1.4895044147\n",
      "B_min=23.6597106889, B_max=27.4502578093\n",
      "C_min=0.2001000969, C_max=0.2010110331\n",
      "min_validation=0.0000810463\n",
      "========================================\n",
      "69 outputs_second_search/solution_10_8_95.npy\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "./temp/res_69_1_percent_noise\n",
      "A_min=0.1190242394, A_max=1.4847814592\n",
      "B_min=23.6994379486, B_max=27.4394819988\n",
      "C_min=0.2001006263, C_max=0.2010042915\n",
      "min_validation=0.0001330843\n",
      "========================================\n",
      "69 outputs_second_search/solution_10_8_95.npy\n",
      "./temp/res_69_2_percent_noise\n",
      "A_min=0.1201484683, A_max=1.4862542632\n",
      "B_min=23.6964056558, B_max=27.4410759683\n",
      "C_min=0.2000980539, C_max=0.2010049747\n",
      "min_validation=0.0004447111\n",
      "========================================\n",
      "69 outputs_second_search/solution_10_8_95.npy\n",
      "./temp/res_69_5_percent_noise\n",
      "A_min=0.1144333108, A_max=1.4935213861\n",
      "B_min=23.6820362097, B_max=27.4546519074\n",
      "C_min=0.2000854222, C_max=0.2010080595\n",
      "min_validation=0.0004654335\n",
      "========================================\n",
      "69 outputs_second_search/solution_10_8_95.npy\n",
      "./temp/res_73_no_noise\n",
      "A_min=0.0501380609, A_max=0.7430282720\n",
      "B_min=23.6160203441, B_max=27.4899110137\n",
      "C_min=0.2011447711, C_max=0.2026245606\n",
      "min_validation=0.0001323765\n",
      "========================================\n",
      "73 outputs_second_search/solution_10_9_110.npy\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function grads at 0x7f543c15b310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "./temp/res_73_1_percent_noise\n",
      "A_min=0.0550695725, A_max=0.7431286863\n",
      "B_min=23.6549444687, B_max=27.4772602006\n",
      "C_min=0.2011549482, C_max=0.2026034489\n",
      "min_validation=0.0000746412\n",
      "========================================\n",
      "73 outputs_second_search/solution_10_9_110.npy\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function grads at 0x7f543c15b310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "./temp/res_73_2_percent_noise\n",
      "A_min=0.0552520872, A_max=0.7460737877\n",
      "B_min=23.6480137927, B_max=27.4798159771\n",
      "C_min=0.2011535938, C_max=0.2026051596\n",
      "min_validation=0.0003709311\n",
      "========================================\n",
      "73 outputs_second_search/solution_10_9_110.npy\n",
      "./temp/res_73_5_percent_noise\n",
      "A_min=0.0507517494, A_max=0.7549090921\n",
      "B_min=23.6272217646, B_max=27.4874833068\n",
      "C_min=0.2011401601, C_max=0.2026181513\n",
      "min_validation=0.0013842260\n",
      "========================================\n",
      "73 outputs_second_search/solution_10_9_110.npy\n",
      "./temp/res_84_no_noise\n",
      "A_min=0.1379534273, A_max=8.2416058427\n",
      "B_min=25.5410542857, B_max=29.1482526587\n",
      "C_min=10.5310966586, C_max=10.8076479823\n",
      "min_validation=0.0000419499\n",
      "========================================\n",
      "84 outputs_second_search/solution_10_9_188.npy\n",
      "./temp/res_84_1_percent_noise\n",
      "A_min=0.1774374374, A_max=8.2416766421\n",
      "B_min=25.5585676993, B_max=29.1374475569\n",
      "C_min=10.5318753688, C_max=10.8057162288\n",
      "min_validation=0.0001675099\n",
      "========================================\n",
      "84 outputs_second_search/solution_10_9_188.npy\n",
      "./temp/res_84_2_percent_noise\n",
      "A_min=0.1890676456, A_max=8.2727336299\n",
      "B_min=25.5528964817, B_max=29.1409378844\n",
      "C_min=10.5317067031, C_max=10.8059909715\n",
      "min_validation=0.0000747072\n",
      "========================================\n",
      "84 outputs_second_search/solution_10_9_188.npy\n",
      "./temp/res_84_5_percent_noise\n",
      "A_min=0.1091402835, A_max=8.3659045933\n",
      "B_min=25.5358828291, B_max=29.1645410284\n",
      "C_min=10.5300806716, C_max=10.8068151997\n",
      "min_validation=0.0004475932\n",
      "========================================\n",
      "84 outputs_second_search/solution_10_9_188.npy\n",
      "./temp/res_44_no_noise\n",
      "A_min=0.0904517381, A_max=9.1282162414\n",
      "B_min=14.7490119213, B_max=20.8363636789\n",
      "C_min=2.0051448842, C_max=2.2960615786\n",
      "min_validation=0.0000365830\n",
      "========================================\n",
      "44 outputs_second_search/solution_10_11_76.npy\n",
      "./temp/res_44_1_percent_noise\n",
      "A_min=0.1036461638, A_max=9.0873889174\n",
      "B_min=14.7630551225, B_max=20.8163388049\n",
      "C_min=2.0048676169, C_max=2.2921263811\n",
      "min_validation=0.0000363943\n",
      "========================================\n",
      "44 outputs_second_search/solution_10_11_76.npy\n",
      "./temp/res_44_2_percent_noise\n",
      "A_min=0.0963454523, A_max=9.0779412729\n",
      "B_min=14.7503843965, B_max=20.8220008982\n",
      "C_min=2.0042515229, C_max=2.2921232053\n",
      "min_validation=0.0000625903\n",
      "========================================\n",
      "44 outputs_second_search/solution_10_11_76.npy\n",
      "./temp/res_44_5_percent_noise\n",
      "A_min=0.0086126863, A_max=9.0608843166\n",
      "B_min=14.7123722183, B_max=20.8475418820\n",
      "C_min=2.0010219839, C_max=2.2924222540\n",
      "min_validation=0.0003611558\n",
      "========================================\n",
      "44 outputs_second_search/solution_10_11_76.npy\n",
      "./temp/res_56_no_noise\n",
      "A_min=0.0330913538, A_max=0.7920515686\n",
      "B_min=8.8902010676, B_max=10.7254766697\n",
      "C_min=0.2011176582, C_max=0.2030878826\n",
      "min_validation=0.0000760525\n",
      "========================================\n",
      "56 outputs_second_search/solution_10_8_140.npy\n",
      "./temp/res_56_1_percent_noise\n",
      "A_min=0.0371946025, A_max=0.7858142974\n",
      "B_min=8.9070802883, B_max=10.7109927683\n",
      "C_min=0.2011264469, C_max=0.2030697953\n",
      "min_validation=0.0003237122\n",
      "========================================\n",
      "56 outputs_second_search/solution_10_8_140.npy\n",
      "./temp/res_56_2_percent_noise\n",
      "A_min=0.0384333921, A_max=0.7866598665\n",
      "B_min=8.9051158124, B_max=10.7146191488\n",
      "C_min=0.2011249387, C_max=0.2030729374\n",
      "min_validation=0.0000611625\n",
      "========================================\n",
      "56 outputs_second_search/solution_10_8_140.npy\n",
      "./temp/res_56_5_percent_noise\n",
      "A_min=0.0390775245, A_max=0.7891965737\n",
      "B_min=8.8927612547, B_max=10.7254982902\n",
      "C_min=0.2011095827, C_max=0.2030823638\n",
      "min_validation=0.0003815364\n",
      "========================================\n",
      "56 outputs_second_search/solution_10_8_140.npy\n",
      "./temp/res_62_no_noise\n",
      "A_min=0.0478547339, A_max=1.6675072460\n",
      "B_min=23.9279902957, B_max=27.1749230407\n",
      "C_min=0.2000046446, C_max=0.2000608040\n",
      "min_validation=0.0002459788\n",
      "========================================\n",
      "62 outputs_second_search/solution_10_8_161.npy\n",
      "./temp/res_62_1_percent_noise\n",
      "A_min=0.0522591638, A_max=1.6592080287\n",
      "B_min=23.9486652576, B_max=27.1513358253\n",
      "C_min=0.2000047793, C_max=0.2000605786\n",
      "min_validation=0.0000590068\n",
      "========================================\n",
      "62 outputs_second_search/solution_10_8_161.npy\n",
      "./temp/res_62_2_percent_noise\n",
      "A_min=0.0497298016, A_max=1.6619129152\n",
      "B_min=23.9478145535, B_max=27.1500717134\n",
      "C_min=0.2000046481, C_max=0.2000605704\n",
      "min_validation=0.0001003832\n",
      "========================================\n",
      "62 outputs_second_search/solution_10_8_161.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./temp/res_62_5_percent_noise\n",
      "A_min=0.0420607470, A_max=1.6853128002\n",
      "B_min=23.9380350592, B_max=27.1534423314\n",
      "C_min=0.2000042031, C_max=0.2000605699\n",
      "min_validation=0.0004901818\n",
      "========================================\n",
      "62 outputs_second_search/solution_10_8_161.npy\n"
     ]
    }
   ],
   "source": [
    "n_max = 20\n",
    "results = []\n",
    "for index in [69, 73, 84, 44, 56, 62]:\n",
    "    for run in [\"no_noise\", \"1_percent_noise\", \"2_percent_noise\", \"5_percent_noise\"]:\n",
    "        print(f\"./temp/res_{index}_{run}\")\n",
    "        pinn = tu.NN.restore(\".\", f\"pinn_final_{index}_{run}\")\n",
    "        # Load max and mins        \n",
    "        with open(f\"./temp/res_{index}_{run}.npy\", \"rb\") as f:\n",
    "            (A_max, A_min, \n",
    "             B_max, B_min, \n",
    "             C_max, C_min, \n",
    "             min_validation)= np.load(f, allow_pickle=True)\n",
    "            print(f\"A_min={A_min:.10f}, A_max={A_max:.10f}\")\n",
    "            print(f\"B_min={B_min:.10f}, B_max={B_max:.10f}\")\n",
    "            print(f\"C_min={C_min:.10f}, C_max={C_max:.10f}\")\n",
    "            print(f\"min_validation={min_validation:.10f}\")\n",
    "\n",
    "        #################\n",
    "        (path,\n",
    "         (final_loss_A, final_params_A, \n",
    "             init_loss_A, init_params_A),\n",
    "            (final_loss_B, final_params_B, \n",
    "             init_loss_B, init_params_B),\n",
    "            (final_loss_C, final_params_C, \n",
    "             init_loss_C, init_params_C)\n",
    "           ) = search_for_params(index, \n",
    "                      pinn,\n",
    "                      A_max, A_min,\n",
    "                      B_max, B_min,\n",
    "                      C_max, C_min,\n",
    "                      n_max,\n",
    "                      slice_min = 45, \n",
    "                      slice_max = 100,)\n",
    "        results.append({\n",
    "          'index':index,\n",
    "          'run':run,\n",
    "          'path': path,\n",
    "          'A_max':A_max, \n",
    "          'A_min':A_min,\n",
    "          'B_max':B_max,\n",
    "          'B_min':B_min,\n",
    "          'C_max':C_max,\n",
    "          'C_min':C_min,\n",
    "          'final_loss_A':final_loss_A,\n",
    "          'final_params_A':final_params_A, \n",
    "          'init_loss_A':init_loss_A,\n",
    "          'init_params_A':init_params_A,\n",
    "          'final_loss_B':final_loss_B, \n",
    "          'final_params_B':final_params_B, \n",
    "          'init_loss_B':init_loss_B,\n",
    "          'init_params_B':init_params_B,\n",
    "          'final_loss_C':final_loss_C, \n",
    "          'final_params_C':final_params_C, \n",
    "          'init_loss_C':init_loss_C, \n",
    "          'init_params_C':init_params_C\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d83a0252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T16:30:51.649055Z",
     "start_time": "2023-07-14T16:30:51.526596Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"./temp/temp_file_2.npy\", \"wb\") as f:\n",
    "    np.save(f, results)\n",
    "# n_max = 20\n",
    "# with open(f\"./temp/temp_file.npy\", \"rb\") as f:\n",
    "#     results = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f531cf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:28:37.019248Z",
     "start_time": "2023-07-10T17:28:37.002053Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "simulate_from_start = False\n",
    "args_list = [\n",
    "    (simulate_from_start,\n",
    "     i,\n",
    "     dict_params[\"index\"],\n",
    "     dict_params[\"run\"],\n",
    "     dict_params[\"path\"],\n",
    "     dict_params[\"final_params_A\"][i], \n",
    "     dict_params[\"final_params_B\"][i], \n",
    "     dict_params[\"final_params_C\"][i]) for dict_params in results\n",
    "                                       for i in range(n_max) \n",
    "]\n",
    "\n",
    "args_list = [ (iter_i, item)\n",
    "    for iter_i, item in enumerate(args_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18390d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:28:51.409103Z",
     "start_time": "2023-07-10T17:28:51.399924Z"
    }
   },
   "outputs": [],
   "source": [
    "shm = shared_memory.SharedMemory(name=\"models_list\",\n",
    "                                 create=True, \n",
    "                                 size=models_list.nbytes)\n",
    "shared_models_list = np.ndarray((pool_num,), \n",
    "                                 dtype=models_list.dtype,\n",
    "                                 buffer=shm.buf)\n",
    "\n",
    "shared_models_list[:] = models_list[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938eecee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:29:50.687877Z",
     "start_time": "2023-07-10T17:28:53.828799Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with Pool(pool_num) as pool:\n",
    "    results2 = pool.map(single_simulation, args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34743099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:30:25.262390Z",
     "start_time": "2023-07-10T19:30:25.254100Z"
    }
   },
   "outputs": [],
   "source": [
    "model_shm = shared_memory.SharedMemory(name=\"models_list\")\n",
    "model_shm.close()\n",
    "model_shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea87b383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T19:33:57.610588Z",
     "start_time": "2023-07-10T19:33:57.179488Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"./temp/temp_file2_2.npy\", \"wb\") as f:\n",
    "    np.save(f, results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38815555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-10T17:12:22.235655Z",
     "start_time": "2023-07-10T17:12:22.226648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0062000124000248"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10 - 10/127)**2/(126*126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c036679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
