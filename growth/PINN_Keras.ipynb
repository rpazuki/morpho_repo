{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cb4846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:01:43.030916Z",
     "start_time": "2022-06-13T18:01:40.138522Z"
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import time\n",
    "import threading\n",
    "\n",
    "GPU = False\n",
    "import os\n",
    "\n",
    "if GPU:\n",
    "    txt_device = 'gpu:0'\n",
    "else:\n",
    "    txt_device = 'cpu:0'    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6700ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:26:42.996946Z",
     "start_time": "2022-06-13T18:26:42.957622Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, layers, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "           layers: input, dense layers and outputs dimensions  \n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(self.layers)        \n",
    "        \n",
    "    def build(self, input_shape):         \n",
    "        \"\"\"Create the state of the layers (weights)\"\"\"\n",
    "        weights = []\n",
    "        biases = []        \n",
    "        for l in range(0,self.num_layers-1):\n",
    "            W = self.xavier_init(size=[self.layers[l], self.layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,self.layers[l+1]], dtype=tf.float64), dtype=tf.float64)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        \n",
    "        self.Ws = weights\n",
    "        self.bs = biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.compat.v1.truncated_normal([in_dim, out_dim], \n",
    "                                                         stddev=xavier_stddev, \n",
    "                                                         dtype=tf.float64), \n",
    "                           dtype=tf.float64)\n",
    "    \n",
    "    def normalise_input(self, inputs):\n",
    "        \"\"\"Map the inputs to the range [-1, 1]\"\"\"\n",
    "        return 2.0*(inputs - self.lb)/(self.ub - self.lb) - 1.0\n",
    "    \n",
    "    @tf.function\n",
    "    def __net__(self, inputs):\n",
    "        H = inputs\n",
    "        for W, b in zip(self.Ws[:-1], self.bs[:-1]):\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            \n",
    "            W = self.Ws[-1]\n",
    "            b = self.bs[-1]\n",
    "            outputs = tf.add(tf.matmul(H, W), b)\n",
    "        return outputs\n",
    "    \n",
    "    def call(self, inputs, grads=True):\n",
    "        \"\"\"Defines the computation from inputs to outputs\"\"\"\n",
    "        X = tf.cast(inputs, tf.float64)\n",
    "        outputs = self.__net__(X)\n",
    "        if grads:                        \n",
    "            partials_1 = [tf.gradients(outputs[:,i], X)[0] for i in range(outputs.shape[1])]\n",
    "            partials_2 = [tf.gradients(partials_1[i], X)[0] for i in range(outputs.shape[1])]\n",
    "            return outputs, partials_1, partials_2            \n",
    "        else:            \n",
    "            return outputs   \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class TINN():\n",
    "    \"\"\"Turing-Informed Neoral Net\"\"\"\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 inputs_obs,\n",
    "                 output_obs,\n",
    "                 inputs_pde = None,\n",
    "                 boundary_spec = dict(\n",
    "                    inputs_boundary = None,\n",
    "                    boundary_loss_callback = None,\n",
    "                    needs_grad = False \n",
    "                   )                 \n",
    "                 ):\n",
    "        self.model = NN(layers)\n",
    "        self.inputs_obs = inputs_obs\n",
    "        self.output_obs = output_obs        \n",
    "        # Use the observation points for validating\n",
    "        # PDE\n",
    "        if inputs_pde is None:\n",
    "            self.inputs_pde = inputs_obs\n",
    "        else:\n",
    "            self.inputs_pde = inputs_pde\n",
    "            \n",
    "        self.boundary_spec = boundary_spec\n",
    "        if boundary_spec['inputs_boundary'] is None:\n",
    "            self.has_bounday = False\n",
    "        else:\n",
    "            self.has_bounday = True\n",
    "        if self.boundary_spec['boundary_loss_callback'] is None:\n",
    "            self.boundary_loss_callback = self.periodic_boundary\n",
    "        else:\n",
    "            self.boundary_loss_callback = self.boundary_spec['boundary_loss_callback']        \n",
    "        \n",
    "        self.optimizer_Adam = keras.optimizers.Adam()\n",
    "        #self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        #self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "     \n",
    "    def periodic_boundary(self, inputs, boundary_pred, boundary_spec):        \n",
    "        pass\n",
    "    \n",
    "    def pde_residuals(self, pde_inputs, pde_outputs, partials_1, partials_2):\n",
    "        pass\n",
    "    \n",
    "    def model_vars(self):\n",
    "        return []\n",
    "    \n",
    "    def loss_obs(self, inputs, outputs):\n",
    "        obs_pred = self.model(inputs, grads = False)\n",
    "        L = tf.reduce_sum(tf.square(obs_pred - outputs), name = \"Loss_observations\")\n",
    "        return L\n",
    "    \n",
    "    def loss_pde(self, inputs):\n",
    "        pde_outputs, partials_1, partials_2 = self.model(inputs, grads = True)\n",
    "        pde_res = self.pde_residuals(inputs, pde_outputs, partials_1, partials_2)\n",
    "        L = tf.reduce_sum(tf.square(pde_res), name = \"Loss_pde\")\n",
    "        return L\n",
    "        \n",
    "    def loss_boundary(self, inputs, outputs):\n",
    "        boundary_pred = self.model(inputs, self.boundary_spec['needs_grad'])        \n",
    "        L = self.boundary_loss_callback(inputs, boundary_pred, self.boundary_spec)        \n",
    "        return L\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __batches__(self, batch_size):\n",
    "        \"\"\"Generator that returned shuffled indeces for each batch\"\"\"\n",
    "        \n",
    "        flg_boundary = self.has_bounday\n",
    "        # Observation batch info \n",
    "        obs_n = self.inputs_obs.shape[0]\n",
    "        batch_steps = obs_n//batch_size\n",
    "        batch_steps = batch_steps + (obs_n-1)//(batch_steps*batch_size)\n",
    "        # PDE batch info\n",
    "        pde_n = self.inputs_pde.shape[0]\n",
    "        pde_batch_size = pde_n//batch_steps        \n",
    "        # Boundary condition batch info\n",
    "        if flg_boundary:\n",
    "            boundary_n = self.boundary_spec.inputs_boundary.shape[0]        \n",
    "            boundary_batch_size = boundary_n//batch_steps\n",
    "        # Observation indices   \n",
    "        indices_obs = np.array(list(range(obs_n)))\n",
    "        np.random.shuffle(indices_obs)\n",
    "        # PDE  indices\n",
    "        indices_pde = np.array(list(range(pde_n)))        \n",
    "        np.random.shuffle(indices_pde)\n",
    "        # Boundary condition  indices\n",
    "        if flg_boundary:\n",
    "            indices_boundary = np.array(list(range(boundary_n)))\n",
    "            np.random.shuffle(indices_boundary)\n",
    "            \n",
    "        for batch in range(batch_steps):\n",
    "            # Observation start-end\n",
    "            obs_start = batch*batch_size\n",
    "            obs_end = (batch+1)*batch_size\n",
    "            obs_end = obs_end - (obs_end//obs_n)*(obs_end%obs_n)\n",
    "            # PDE  start-end\n",
    "            pde_start = batch*pde_batch_size\n",
    "            pde_end = (batch+1)*pde_batch_size            \n",
    "            # Boundary condition  start-end\n",
    "            if flg_boundary:\n",
    "                boundary_start = batch*boundary_batch_size\n",
    "                boundary_end = (batch+1)*boundary_batch_size\n",
    "                \n",
    "            # Correction for PDE and boundary batches at last step\n",
    "            if batch == batch_steps-1:\n",
    "                if pde_end < pde_n:\n",
    "                    pde_end = pde_n\n",
    "                        \n",
    "                if flg_boundary and boundary_end < boundary_n:\n",
    "                    boundary_end = boundary_n\n",
    "            # step's indices        \n",
    "            batch_indices_obs = indices_obs[obs_start:obs_end]\n",
    "            batch_indices_pde = indices_pde[pde_start:pde_end]\n",
    "            if flg_boundary:\n",
    "                batch_indices_boundary = indices_boundary[boundary_start:boundary_end]\n",
    "                yield (batch_indices_obs, batch_indices_pde, batch_indices_boundary)\n",
    "            else:\n",
    "                yield (batch_indices_obs, batch_indices_pde, None)\n",
    "        \n",
    "    def train(self, epochs, batch_size, print_iter=10):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0            \n",
    "            for obs_indeces, pde_indeces, boundary_indices in self.__batches__(batch_size):                \n",
    "                obs_inputs_batch  = self.inputs_obs[obs_indeces]\n",
    "                obs_outputs_batch = self.output_obs[obs_indeces]\n",
    "                pde_inputs_batch  = self.inputs_pde[pde_indeces] \n",
    "                if self.has_bounday:\n",
    "                    boundary_inputs_batch = self.boundary_spec.inputs_boundary[boundary_indices]\n",
    "                    L = self.train_step(obs_inputs_batch, \n",
    "                                        obs_outputs_batch, \n",
    "                                        pde_inputs_batch,\n",
    "                                        boundary_inputs_batch)\n",
    "                else:\n",
    "                    L = self.train_step(obs_inputs_batch, \n",
    "                                        obs_outputs_batch, \n",
    "                                        pde_inputs_batch,\n",
    "                                        None)\n",
    "                total_loss += L\n",
    "                \n",
    "            if epoch % print_iter == 0:\n",
    "                elapsed = time.time() - start_time                                                                \n",
    "                print(f\"Epoch: {epoch}, loss:{total_loss:.2f}, \\n\"\n",
    "                      f\"Time:{elapsed:.2f}\\n\")\n",
    "                start_time = time.time()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, \n",
    "                   obs_inputs_batch, \n",
    "                   obs_outputs_batch, \n",
    "                   pde_inputs_batch,\n",
    "                   boundary_inputs_batch):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_obs = self.loss_obs(obs_inputs_batch, obs_outputs_batch)\n",
    "            loss_pde = self.loss_pde(pde_inputs_batch)\n",
    "                    \n",
    "            loss = loss_obs + loss_pde\n",
    "            if self.has_bounday:\n",
    "                loss += self.loss_boundary(boundary_inputs_batch)\n",
    "                \n",
    "        \n",
    "        trainable_vars = self.model.trainable_weights + self.model_vars()\n",
    "        grads = tape.gradient(loss,  trainable_vars)\n",
    "        self.optimizer_Adam.apply_gradients(zip(grads, trainable_vars))        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "class ASDM(TINN):\n",
    "    def __init__(self,\n",
    "                 *arg,\n",
    "                 **kwargs):\n",
    "        super().__init__(*arg, **kwargs)\n",
    "        \n",
    "        \n",
    "        self.sigma_a = tf.Variable([0.0], dtype=tf.float64,\n",
    "                                   name=\"sigma_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.sigma_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                   name=\"sigma_s\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.mu_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                name=\"mu_a\",\n",
    "                                constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_a\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_s\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.kappa_a = tf.Variable([1.00], dtype=tf.float64,\n",
    "                                   name=\"kappa_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        \n",
    "    def model_vars(self):\n",
    "        return [self.sigma_a,\n",
    "                self.sigma_s,\n",
    "                self.mu_a,\n",
    "                self.rho_a,\n",
    "                self.rho_s,\n",
    "                self.kappa_a\n",
    "               ]\n",
    "    \n",
    "    def pde_residuals(self, pde_inputs, pde_outputs, partials_1, partials_2):    \n",
    "        \n",
    "        a = pde_outputs[:, 0]\n",
    "        s = pde_outputs[:, 1]\n",
    "        \n",
    "        a_x = partials_1[0][:, 0]\n",
    "        a_y = partials_1[0][:, 1]\n",
    "        a_t = partials_1[0][:, 2]\n",
    "        \n",
    "        a_xx = partials_2[0][:, 0]\n",
    "        a_yy = partials_2[0][:, 1]\n",
    "        \n",
    "        \n",
    "        s_x = partials_1[1][:, 0]\n",
    "        s_y = partials_1[1][:, 1]\n",
    "        s_t = partials_1[1][:, 2]\n",
    "        \n",
    "        s_xx = partials_2[1][:, 0]\n",
    "        s_yy = partials_2[1][:, 1]\n",
    "        \n",
    "        sigma_a = self.sigma_a\n",
    "        sigma_s = self.sigma_s\n",
    "        mu_a = self.mu_a\n",
    "        rho_a = self.rho_a\n",
    "        rho_s = self.rho_s\n",
    "        kappa_a = self.kappa_a\n",
    "        \n",
    "        one_1 = tf.constant(1.0, dtype=tf.float64)\n",
    "        f = a*a*s/(one_1 + kappa_a*a*a)\n",
    "        f_a = a_t - (a_xx + a_yy) - rho_a*f + mu_a*a - sigma_a\n",
    "        f_s = s_t - (s_xx + s_yy) + rho_s*f - sigma_s\n",
    "        \n",
    "        return tf.concat([tf.expand_dims(f_a,axis=1), \n",
    "                          tf.expand_dims(f_s,axis=1),], axis = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37665e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:22:04.308243Z",
     "start_time": "2022-06-13T18:22:03.987972Z"
    }
   },
   "outputs": [],
   "source": [
    "def lower_upper_bounds(inputs_of_inputs):\n",
    "    \"\"\"Find the lower and upper bounds of inputs\n",
    "    \n",
    "       inputs_of_inputs: a list of tensors that their axis one have the same number \n",
    "                         of columns\n",
    "    \"\"\"\n",
    "           \n",
    "    inputs_dim = np.asarray(inputs_of_inputs[0]).shape[1]\n",
    "    lb = np.array([np.inf] * inputs_dim)\n",
    "    ub = np.array([-np.inf] * inputs_dim)\n",
    "    for i, inputs in enumerate(inputs_of_inputs):        \n",
    "        assert inputs_dim == np.asarray(inputs).shape[1]\n",
    "        lb = np.amin(np.c_[inputs.min(0), lb], 1)\n",
    "        ub = np.amax(np.c_[inputs.max(0), ub], 1)\n",
    "        \n",
    "    return lb, ub\n",
    "    \n",
    "def normalise_inputs(inputs_of_inputs):\n",
    "    \"\"\"Scales the values along axis 1 to [-1, 1]\n",
    "    \n",
    "       inputs_of_inputs: a list of tensors that their axis one have the same number \n",
    "                         of columns\n",
    "    \"\"\"\n",
    "    if type(inputs_of_inputs) is not list:\n",
    "        inputs_of_inputs = [inputs_of_inputs]        \n",
    "            \n",
    "    lb, ub = lower_upper_bounds(inputs_of_inputs)\n",
    "    return [2.0*(inputs-lb)/(ub-lb) - 1.0 for inputs in inputs_of_inputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fe7e4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:20:16.445679Z",
     "start_time": "2022-06-13T18:20:16.339683Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [3, 10, 10, 10, 2]\n",
    "sample_xyt  = np.array([ [0, 0, 0],\n",
    "                         [0, 1, 0],\n",
    "                         [1, 0, 0],\n",
    "                         [1, 1, 0],\n",
    "                         [0, 0, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [1, 0, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [0, 0, 2],\n",
    "                         [0, 1, 2],\n",
    "                         [1, 0, 2],\n",
    "                         [1, 1, 2]\n",
    "                       ])\n",
    "\n",
    "#lb, ub = lower_upper_bounds(sample_xyt)\n",
    "#print(normalise_inputs([sample_xyt, sample_xyt.copy()*2]))\n",
    "\n",
    "model = NN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8e9ae9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:11:42.070563Z",
     "start_time": "2022-06-13T18:11:41.894016Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling layer \"nn_7\" (type NN).\n\ntf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\n\nCall arguments received by layer \"nn_7\" (type NN):\n  • inputs=tf.Tensor(shape=(12, 3), dtype=float32)\n  • grads=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from tensorflow.python.framework.ops import disable_eager_execution\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#disable_eager_execution()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m i \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(sample_xyt\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m o, p1, p2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/tensorflow.2.9/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mNN.call\u001b[0;34m(self, inputs, grads)\u001b[0m\n\u001b[1;32m     52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__net__(X)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grads:                        \n\u001b[0;32m---> 54\u001b[0m     partials_1 \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mgradients(outputs[:,i], X)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     55\u001b[0m     partials_2 \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mgradients(partials_1[i], X)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, partials_1, partials_2            \n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__net__(X)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grads:                        \n\u001b[0;32m---> 54\u001b[0m     partials_1 \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     55\u001b[0m     partials_2 \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mgradients(partials_1[i], X)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, partials_1, partials_2            \n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling layer \"nn_7\" (type NN).\n\ntf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\n\nCall arguments received by layer \"nn_7\" (type NN):\n  • inputs=tf.Tensor(shape=(12, 3), dtype=float32)\n  • grads=True"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "#disable_eager_execution()\n",
    "i = tf.constant(sample_xyt*1.0)\n",
    "o, p1, p2 = model(i, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "8621cf0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:00:30.150040Z",
     "start_time": "2022-06-13T18:00:30.146532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'nn_83/gradients/nn_83/StatefulPartitionedCall_grad/PartitionedCall:0' shape=(12, 3) dtype=float64>,\n",
       " <tf.Tensor 'nn_83/gradients_1/nn_83/StatefulPartitionedCall_grad/PartitionedCall:0' shape=(12, 3) dtype=float64>]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "738c241b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:20:19.527604Z",
     "start_time": "2022-06-13T18:20:19.476944Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [3, 64, 64, 64, 64, 2]\n",
    "#layers = [3, 128, 128, 128, 128, 2]\n",
    "\n",
    "#layers = [3, 100, 100, 100, 100, 2]\n",
    "\n",
    "# Load Data\n",
    "import os\n",
    "data_path = os.path.abspath(\"turing.npy\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = np.load(f)\n",
    "    \n",
    "data_path = os.path.abspath(\"turing_t.npy\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    t_star = np.load(f) \n",
    "    \n",
    "T = t_star.shape[0]    \n",
    "    \n",
    "L = 50\n",
    "x_size = data.shape[1]\n",
    "y_size = data.shape[2]\n",
    "N = x_size*y_size\n",
    "x_domain = L*np.linspace(0,1,x_size)\n",
    "y_domain = L*np.linspace(0,1,y_size)\n",
    "\n",
    "X,Y = np.meshgrid(x_domain, y_domain, sparse=False, indexing='ij')\n",
    "XX = np.tile(X.flatten(), T) # N x T\n",
    "YY = np.tile(Y.flatten(), T) # N x T\n",
    "TT = np.repeat(t_star[-T:], N) # T x N\n",
    "\n",
    "AA = np.einsum('ijk->kij', data[0, :, :, -T:]).flatten() # N x T\n",
    "SS = np.einsum('ijk->kij', data[1, :, :, -T:]).flatten() # N x T\n",
    "\n",
    "\n",
    "x = XX[:, np.newaxis] # NT x 1\n",
    "y = YY[:, np.newaxis] # NT x 1\n",
    "t = TT[:, np.newaxis] # NT x 1\n",
    "\n",
    "a = AA[:, np.newaxis] # NT x 1\n",
    "s = SS[:, np.newaxis] # NT x 1\n",
    "\n",
    "boundary_x_LB = np.concatenate((x_domain, \n",
    "                                np.repeat(x_domain[0], y_size)))\n",
    "boundary_x_TR = np.concatenate((x_domain, \n",
    "                                np.repeat(x_domain[-1], y_size))) \n",
    "\n",
    "boundary_y_LB = np.concatenate((np.repeat(y_domain[0], x_size),\n",
    "                                y_domain))\n",
    "boundary_y_TR = np.concatenate((np.repeat(y_domain[-1], x_size),\n",
    "                                y_domain)) \n",
    "\n",
    "boundary_XX_LB = np.tile(boundary_x_LB.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_XX_TR = np.tile(boundary_x_TR.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_YY_LB = np.tile(boundary_y_LB.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_YY_TR = np.tile(boundary_y_TR.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_TT = np.repeat(t_star[-T:], (x_size + y_size))[:, np.newaxis] # T x (x_size + y_size), 1\n",
    "\n",
    "\n",
    "def create_dataset(training_data_size =  T*16,\n",
    "                   pde_data_size =  (T*N)//(32),\n",
    "                   boundary_data_size = ((x_size + y_size)*T)//(8),\n",
    "                   with_boundary = True,\n",
    "                   signal_to_noise = 0):\n",
    "    \n",
    "    ##########################################\n",
    "    # Including noise\n",
    "    if signal_to_noise > 0:\n",
    "        signal_amp_a = (np.max(AA)-np.min(AA))/2.0\n",
    "        signal_amp_s = (np.max(SS)-np.min(SS))/2.0  \n",
    "        sigma_a =  signal_amp_a*signal_to_noise\n",
    "        sigma_s =  signal_amp_s*signal_to_noise\n",
    "    # Observed data\n",
    "    idx_data = np.random.choice(N*T, training_data_size, replace=False)\n",
    "    # PDE colocations\n",
    "    idx_pde = np.random.choice(N*T, pde_data_size, replace=False)\n",
    "    # Periodic boundary condition\n",
    "    idx_boundary = np.random.choice((x_size + y_size)*T, boundary_data_size, replace=False)\n",
    "    \n",
    "    ret = {'x_obs': x[idx_data,:],\n",
    "            'y_obs': y[idx_data,:],\n",
    "            't_obs': t[idx_data,:],\n",
    "            'a_obs': a[idx_data,:],\n",
    "            's_obs': s[idx_data,:],\n",
    "            'x_pde':   x[idx_pde,:],\n",
    "            'y_pde':   y[idx_pde,:],\n",
    "            't_pde':   t[idx_pde,:]}\n",
    "    \n",
    "    if signal_to_noise > 0:        \n",
    "        ret['a_obs'] += sigma_a * np.random.randn(len(idx_data), a.shape[1])\n",
    "        ret['s_obs'] += sigma_s * np.random.randn(len(idx_data), s.shape[1])\n",
    "    \n",
    "    if with_boundary:\n",
    "        ret = {**ret,\n",
    "               **{'x_boundary_LB': boundary_XX_LB[idx_boundary],\n",
    "                  'x_boundary_TR': boundary_XX_TR[idx_boundary],\n",
    "                  'y_boundary_LB': boundary_YY_LB[idx_boundary],\n",
    "                  'y_boundary_TR': boundary_YY_TR[idx_boundary],\n",
    "                  't_boundary_LB': boundary_TT[idx_boundary],\n",
    "                  't_boundary_TR': boundary_TT[idx_boundary]}\n",
    "              }\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0104acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:20:20.131920Z",
     "start_time": "2022-06-13T18:20:20.092314Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params_1 = {'training_data_size': T*16,\n",
    "                'pde_data_size': (T*N)//(32),\n",
    "                'boundary_data_size':((x_size + y_size)*T)//(8)}\n",
    "\n",
    "dataset = create_dataset(**model_params_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e0cec75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:26:45.104615Z",
     "start_time": "2022-06-13T18:26:45.095968Z"
    }
   },
   "outputs": [],
   "source": [
    "asdm = ASDM(layers, \n",
    "           inputs_obs = np.c_[dataset['x_obs'], dataset['y_obs'], dataset['t_obs']],\n",
    "           output_obs = np.c_[dataset['a_obs'], dataset['s_obs']]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1ce2f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:27:18.260569Z",
     "start_time": "2022-06-13T18:26:50.155339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss:35680.41, \n",
      "Time:1.70\n",
      "\n",
      "Epoch: 100, loss:2178.56, \n",
      "Time:6.64\n",
      "\n",
      "Epoch: 200, loss:2176.05, \n",
      "Time:6.57\n",
      "\n",
      "Epoch: 300, loss:2176.39, \n",
      "Time:6.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asdm.train(400, T*4, print_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1a3a469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:27:19.832997Z",
     "start_time": "2022-06-13T18:27:19.820377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sigma_a:0' shape=(1,) dtype=float64, numpy=array([0.10185468])>,\n",
       " <tf.Variable 'sigma_s:0' shape=(1,) dtype=float64, numpy=array([0.83944022])>,\n",
       " <tf.Variable 'mu_a:0' shape=(1,) dtype=float64, numpy=array([0.88263012])>,\n",
       " <tf.Variable 'rho_a:0' shape=(1,) dtype=float64, numpy=array([1.12305397])>,\n",
       " <tf.Variable 'rho_s:0' shape=(1,) dtype=float64, numpy=array([1.19049248])>,\n",
       " <tf.Variable 'kappa_a:0' shape=(1,) dtype=float64, numpy=array([0.82351467])>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdm.model_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63755b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
