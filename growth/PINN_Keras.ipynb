{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cb4846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T22:40:18.560859Z",
     "start_time": "2022-06-13T22:40:15.713774Z"
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import time\n",
    "import threading\n",
    "\n",
    "GPU = False\n",
    "import os\n",
    "\n",
    "if GPU:\n",
    "    txt_device = 'gpu:0'\n",
    "else:\n",
    "    txt_device = 'cpu:0'    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "b6700ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:43:36.589911Z",
     "start_time": "2022-06-14T14:43:36.530134Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, layers, lb, ub, **kwargs):\n",
    "        \"\"\"A dense Neural Net that is specified by layers argument.\n",
    "        \n",
    "           layers: input, dense layers and outputs dimensions  \n",
    "           lb    : An array of minimums of inputs (lower bounds)\n",
    "           ub    : An array of maximums of inputs (upper bounds)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(self.layers)        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "    def build(self, input_shape):         \n",
    "        \"\"\"Create the state of the layers (weights)\"\"\"\n",
    "        weights = []\n",
    "        biases = []        \n",
    "        for l in range(0,self.num_layers-1):\n",
    "            W = self.xavier_init(size=[self.layers[l], self.layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,self.layers[l+1]], dtype=tf.float64), dtype=tf.float64)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        \n",
    "        self.Ws = weights\n",
    "        self.bs = biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.compat.v1.truncated_normal([in_dim, out_dim], \n",
    "                                                         stddev=xavier_stddev, \n",
    "                                                         dtype=tf.float64), \n",
    "                           dtype=tf.float64)\n",
    "    \n",
    "    @tf.function\n",
    "    def normalise_input(self, inputs):\n",
    "        \"\"\"Map the inputs to the range [-1, 1]\"\"\"\n",
    "        return 2.0*(inputs - self.lb)/(self.ub - self.lb) - 1.0\n",
    "    \n",
    "    @tf.function\n",
    "    def __net__(self, inputs):\n",
    "        H = self.normalise_input(inputs)\n",
    "        for W, b in zip(self.Ws[:-1], self.bs[:-1]):\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            \n",
    "            W = self.Ws[-1]\n",
    "            b = self.bs[-1]\n",
    "            outputs = tf.add(tf.matmul(H, W), b)\n",
    "        return outputs\n",
    "    \n",
    "    def call(self, inputs, grads=True):\n",
    "        \"\"\"Defines the computation from inputs to outputs\n",
    "        \n",
    "        Args:\n",
    "           inputs: A tensor that has a shape [None, D1], where\n",
    "                   D1 is the input dimensionality, specified in\n",
    "                   the first element of layes.\n",
    "           grads:  Default 'True'. Returns the first and second \n",
    "                   order gradients of the output with respect to \n",
    "                   the input when the grads argument is 'True'.\n",
    "                   \n",
    "        Return:\n",
    "                A tensor of the dense layer output that has a shape\n",
    "                [None, Dn], where Dn is the dimensionality of the last\n",
    "                layer, specificed by the last elements of the layer\n",
    "                arguemnt.\n",
    "                When 'grads=True', the list of first and second order gradients \n",
    "                of the output with respect to the input. \n",
    "                \n",
    "        The returns 'partial_1' and 'partial_2' are the first and second \n",
    "        order gradients, repsectivly. Each one is a list that its elements\n",
    "        corresponds to on of the NN's last layer output. e.g. if the last layer\n",
    "        has Dn outputs, each list has Dn tensors as an elements. The dimensionality\n",
    "        of the tensors are the same as inputs: [None, D1]\n",
    "        \n",
    "        \"\"\"\n",
    "        X = tf.cast(inputs, tf.float64)\n",
    "        outputs = self.__net__(X)\n",
    "        if grads:                        \n",
    "            partials_1 = [tf.gradients(outputs[:,i], X)[0] for i in range(outputs.shape[1])]\n",
    "            partials_2 = [tf.gradients(partials_1[i], X)[0] for i in range(outputs.shape[1])]\n",
    "            return outputs, partials_1, partials_2            \n",
    "        else:            \n",
    "            return outputs   \n",
    "    \n",
    "    \n",
    "class Loss():\n",
    "    def __init__(self, pinn, name, data_size = 0, init_loss_weight = 1.0):\n",
    "        \"\"\"Loss value that is calulated for the output of the pinn\n",
    "        \n",
    "        Args:\n",
    "            pinn: NN object that will be trained.\n",
    "            name: The name of the Loss\n",
    "            data_size: The length of its internal dataset.\n",
    "            init_loss_weight: Initial weigth of the loss in comparision to others.\n",
    "            \n",
    "            \n",
    "            When the Loss class has some internal data (e.g. inputs),\n",
    "            it must provid the length of its data_size, since that value\n",
    "            will be used to create randomly shuffled indices on btach training\n",
    "            time.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.pinn = pinn\n",
    "        self.data_size = data_size\n",
    "        self.name = name      \n",
    "        self.init_loss_weight = init_loss_weight\n",
    "    \n",
    "    \n",
    "    def batch(self, indices):\n",
    "        \"\"\"Returns a batch that will be proccessed in loss method\n",
    "        \n",
    "        Args:\n",
    "           indices: Randomly shuffled indices for the current batch.\n",
    "           \n",
    "           Each loss class is responsible for its data. However, the \n",
    "           batch indices are provided by TINN class. So, this method\n",
    "           slices the data that it will use in loss calculation (loss method)\n",
    "           Note that whatever returns fromthis method will be the batch argument\n",
    "           of the loss method, which is casted as Tensor by tensorflow.\n",
    "           \n",
    "           Example:\n",
    "             return (self.input_1[indices], self.input_2[indices])\n",
    "           \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @tf.function\n",
    "    def loss(self, batch):\n",
    "        \"\"\"A tensorflow function that calculates and returns the loss\n",
    "        \n",
    "        Args:\n",
    "           batch: This is the value(s) that is returned from batch method.\n",
    "                  Note that the values are converted to Tensors by Tensorflow\n",
    "                  \n",
    "           It must return the loss values\n",
    "           \n",
    "           Example:\n",
    "              input_1, input_2 = batch\n",
    "              ouput_1 = self.pinn(input_1)\n",
    "              ouput_2 = self.pinn(input_2)\n",
    "              L = tf.reduce_sum(tf.square(output_1 - output_1), name=self.name)\n",
    "              return L\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def loss_weight(self, iteration):\n",
    "        \"\"\"Return weigth of the loss in comparision to others\n",
    "        \n",
    "        Args: \n",
    "            iteration: the iteration index ( for hyper-parametrs that update\n",
    "                       during the training)\n",
    "        \"\"\"\n",
    "        return self.init_loss_weight\n",
    "    \n",
    "    def trainable_vars(self):\n",
    "        \"\"\"Retruns a list of Tensorflow variables for training\n",
    "        \n",
    "           If the loss class has some tarinable variables, it can\n",
    "           return them as a list. These variables will be updated \n",
    "           by optimiser, if they are already part of the computation \n",
    "           graph.\n",
    "        \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def trainable_vars_str(self):  \n",
    "        s = \"\"\n",
    "        t_vars = self.trainable_vars()\n",
    "        if len(t_vars) > 0:\n",
    "            s +=  \", \".join([ f\"{v.name}:{self.__get_val__(v):.8f}\" for v in t_vars])\n",
    "        return s\n",
    "    \n",
    "    def __get_val__(self, item):\n",
    "        val = item.numpy()\n",
    "        if type(val) is float:\n",
    "            return val\n",
    "        else:\n",
    "            return val[0]\n",
    "        \n",
    "        \n",
    "    \n",
    "class Observations(Loss):\n",
    "    def __init__(self, pinn, inputs_obs, outputs_obs):        \n",
    "        self.inputs_obs = inputs_obs\n",
    "        self.outputs_obs = outputs_obs\n",
    "        super().__init__(pinn, \"Loss_observations\", inputs_obs.shape[0])\n",
    "        \n",
    "    def batch(self, indices):\n",
    "        return (self.inputs_obs[indices], self.outputs_obs[indices])\n",
    "    \n",
    "    @tf.function\n",
    "    def loss(self, batch):\n",
    "        inputs, outputs = batch\n",
    "        obs_pred = self.pinn(inputs, grads = False)\n",
    "        L = tf.reduce_sum(tf.square(obs_pred - outputs), name = self.name)\n",
    "        return L\n",
    "    \n",
    "class Periodic_boundary(Loss):\n",
    "    def __init__(self, pinn, inputs_LB_boundary, inputs_TR_boundary):        \n",
    "        self.inputs_LB_boundary = inputs_LB_boundary\n",
    "        self.inputs_TR_boundary = inputs_TR_boundary\n",
    "        super().__init__(pinn, \"Loss_Periodic_Boundary\", inputs_LB_boundary.shape[0])\n",
    "    \n",
    "    def batch(self, indices):\n",
    "        return self.inputs_LB_boundary[indices], self.inputs_TR_boundary[indices]\n",
    "    \n",
    "    @tf.function\n",
    "    def loss(self, batch):\n",
    "        inputs_LB, inputs_TR = batch\n",
    "        boundary_LB_pred = self.pinn(inputs_LB, grads = False)\n",
    "        boundary_TR_pred = self.pinn(inputs_TR, grads = False)\n",
    "        L = tf.reduce_sum(tf.square(boundary_LB_pred - boundary_TR_pred), \n",
    "                          name = self.name)\n",
    "        return L\n",
    "    \n",
    "class Truing_PDE(Loss):\n",
    "    def __init__(self, pinn, inputs_pde, name=\"Loss_Turing_PDE\"):        \n",
    "        self.inputs_pde = inputs_pde\n",
    "        super().__init__(pinn, name ,inputs_pde.shape[0])\n",
    "        \n",
    "    def batch(self, indices):\n",
    "        return self.inputs_pde[indices]\n",
    "       \n",
    "    @tf.function\n",
    "    def loss(self, batch):\n",
    "        inputs = batch\n",
    "        pde_outputs, partials_1, partials_2 = self.pinn(inputs, grads = True)\n",
    "        \n",
    "        pde_res = self.pde(pde_outputs, partials_1, partials_2)\n",
    "        L = tf.reduce_sum(tf.square(pde_res), name = self.name)\n",
    "        return L\n",
    "    \n",
    "    def pde(self, outputs, partials_1, partials_2):\n",
    "        pass\n",
    "    \n",
    "class ASDM(Truing_PDE):\n",
    "    def __init__(self, pinn, inputs_pde):\n",
    "        super().__init__(pinn, inputs_pde, name=\"Loss_ASDM\")\n",
    "        self.sigma_a = tf.Variable([0.0], dtype=tf.float64,\n",
    "                                   name=\"sigma_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.sigma_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                   name=\"sigma_s\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.mu_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                name=\"mu_a\",\n",
    "                                constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_a\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_s\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.kappa_a = tf.Variable([1.00], dtype=tf.float64,\n",
    "                                   name=\"kappa_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        \n",
    "    def trainable_vars(self):\n",
    "        return [self.sigma_a,\n",
    "                self.sigma_s,\n",
    "                self.mu_a,\n",
    "                self.rho_a,\n",
    "                self.rho_s,\n",
    "                self.kappa_a]\n",
    "        \n",
    "    def pde(self, outputs, partials_1, partials_2):\n",
    "        a = outputs[:, 0]\n",
    "        s = outputs[:, 1]\n",
    "        \n",
    "        a_x = partials_1[0][:, 0]\n",
    "        a_y = partials_1[0][:, 1]\n",
    "        a_t = partials_1[0][:, 2]\n",
    "        \n",
    "        a_xx = partials_2[0][:, 0]\n",
    "        a_yy = partials_2[0][:, 1]\n",
    "        \n",
    "        \n",
    "        s_x = partials_1[1][:, 0]\n",
    "        s_y = partials_1[1][:, 1]\n",
    "        s_t = partials_1[1][:, 2]\n",
    "        \n",
    "        s_xx = partials_2[1][:, 0]\n",
    "        s_yy = partials_2[1][:, 1]\n",
    "        \n",
    "        sigma_a = self.sigma_a\n",
    "        sigma_s = self.sigma_s\n",
    "        mu_a = self.mu_a\n",
    "        rho_a = self.rho_a\n",
    "        rho_s = self.rho_s\n",
    "        kappa_a = self.kappa_a\n",
    "        \n",
    "        one_1 = tf.constant(1.0, dtype=tf.float64)\n",
    "        f = a*a*s/(one_1 + kappa_a*a*a)\n",
    "        f_a = a_t - (a_xx + a_yy) - rho_a*f + mu_a*a - sigma_a\n",
    "        f_s = s_t - (s_xx + s_yy) + rho_s*f - sigma_s\n",
    "        \n",
    "        return tf.concat([tf.expand_dims(f_a,axis=1), \n",
    "                          tf.expand_dims(f_s,axis=1),], axis = 1)\n",
    "    \n",
    "class TINN():\n",
    "    \"\"\"Turing-Informed Neoral Net\"\"\"\n",
    "    def __init__(self,\n",
    "                 pinn,\n",
    "                 losses: Loss):\n",
    "        self.pinn = pinn\n",
    "        self.losses = losses        \n",
    "        self.optimizer_Adam = keras.optimizers.Adam()\n",
    "        \n",
    "        \n",
    "    def __indices__(self, batch_size, *ns):\n",
    "        \"\"\"Generator of indices for specified sizes\"\"\"\n",
    "        n1 = ns[0]\n",
    "        ns_remain = ns[1:] if len(ns) > 1 else []                \n",
    "        # First indices        \n",
    "        batch_steps = n1//batch_size\n",
    "        batch_steps = batch_steps + (n1-1)//(batch_steps*batch_size)\n",
    "        # remaining indices        \n",
    "        indices_batch_size = [n_i//batch_steps for n_i in ns_remain]         \n",
    "        # indices\n",
    "        indices = [np.array(list(range(n_i))) for n_i in ns]        \n",
    "        for arr in indices:\n",
    "            np.random.shuffle(arr)\n",
    "            \n",
    "        for batch in range(batch_steps):\n",
    "            # Observation start-end\n",
    "            n1_start = batch*batch_size\n",
    "            n1_end = (batch+1)*batch_size\n",
    "            n1_end = n1_end - (n1_end//n1)*(n1_end%n1)\n",
    "            # remaining indices\n",
    "            starts = [batch*size for size in indices_batch_size]\n",
    "            ends = [(batch+1)*size for size in indices_batch_size]                                       \n",
    "            # Correction for remining indices\n",
    "            if batch == batch_steps-1:\n",
    "                ends = [end if end < ns[i] else ns[i]  for i, end in enumerate(ns_remain)]\n",
    "            # step's indices            \n",
    "            yield [indices[0][n1_start:n1_end]] + \\\n",
    "                  [indices[i+1][star:end] for i, (star, end) in enumerate(zip(starts, ends))]\n",
    "                   \n",
    "    def train(self, epochs, batch_size, print_iter=10):\n",
    "        \n",
    "        datasets_sizes = [ item.data_size for item in self.losses] \n",
    "        samples_total_loss = np.zeros(epochs)\n",
    "        samples_losses = np.zeros((epochs,len(self.losses)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            loss_vals = np.zeros(len(self.losses))\n",
    "\n",
    "            for indices_list in self.__indices__(batch_size, *datasets_sizes):\n",
    "                batches_list = [ l.batch(indices) for l, indices in zip(self.losses,indices_list)]                 \n",
    "                batch_total_loss, batch_loss_vals = self.train_step(batches_list, epoch)\n",
    "                total_loss += batch_total_loss\n",
    "                loss_vals += [l.numpy() for l in batch_loss_vals]\n",
    "\n",
    "                \n",
    "            if epoch % print_iter == 0:\n",
    "                elapsed = time.time() - start_time                                                                \n",
    "                print(f\"Epoch: {epoch}, loss:{total_loss:.2f}\\n\" + \\\n",
    "                      f\"\\n\".join([ f\"{l.name}:{val:.8f} {l.trainable_vars_str()}\" \n",
    "                                  for l, val in zip(self.losses, loss_vals)] ) +\\\n",
    "                      f\"\\nTime:{elapsed:.2f}\\n\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "            samples_total_loss[epoch] = total_loss\n",
    "            samples_losses[epoch, : ] = loss_vals\n",
    "            \n",
    "        return (samples_total_loss, samples_losses)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, batches_list, iteration):        \n",
    "            \n",
    "        with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:            \n",
    "            losses = []\n",
    "            for l, batch in zip(self.losses, batches_list):\n",
    "                L = l.loss(batch)*l.loss_weight(iteration)\n",
    "                losses += [L]\n",
    "                \n",
    "            batch_loss = tf.reduce_sum(losses, name=\"Total_batch_loss\")\n",
    "            \n",
    "        #grads = tape.gradient(batch_loss,  self.trainable_vars)\n",
    "        grads = tape.gradient(batch_loss,  tape.watched_variables())\n",
    "        #self.optimizer_Adam.apply_gradients(zip(grads, self.trainable_vars))\n",
    "        self.optimizer_Adam.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "        \n",
    "        return batch_loss, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "37665e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:43:37.469001Z",
     "start_time": "2022-06-14T14:43:37.457870Z"
    }
   },
   "outputs": [],
   "source": [
    "def lower_upper_bounds(inputs_of_inputs):\n",
    "    \"\"\"Find the lower and upper bounds of inputs\n",
    "    \n",
    "       inputs_of_inputs: a list of tensors that their axis one have the same number \n",
    "                         of columns\n",
    "    \"\"\"\n",
    "           \n",
    "    inputs_dim = np.asarray(inputs_of_inputs[0]).shape[1]\n",
    "    lb = np.array([np.inf] * inputs_dim)\n",
    "    ub = np.array([-np.inf] * inputs_dim)\n",
    "    for i, inputs in enumerate(inputs_of_inputs):        \n",
    "        assert inputs_dim == np.asarray(inputs).shape[1]\n",
    "        lb = np.amin(np.c_[inputs.min(0), lb], 1)\n",
    "        ub = np.amax(np.c_[inputs.max(0), ub], 1)\n",
    "        \n",
    "    return lb, ub\n",
    "    \n",
    "def normalise_inputs(inputs_of_inputs):\n",
    "    \"\"\"Scales the values along axis 1 to [-1, 1]\n",
    "    \n",
    "       inputs_of_inputs: a list of tensors that their axis one have the same number \n",
    "                         of columns\n",
    "    \"\"\"\n",
    "    if type(inputs_of_inputs) is not list:\n",
    "        inputs_of_inputs = [inputs_of_inputs]        \n",
    "            \n",
    "    lb, ub = lower_upper_bounds(inputs_of_inputs)\n",
    "    return [2.0*(inputs-lb)/(ub-lb) - 1.0 for inputs in inputs_of_inputs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "738c241b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:43:39.085828Z",
     "start_time": "2022-06-14T14:43:39.025513Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [3, 64, 64, 64, 64, 2]\n",
    "#layers = [3, 128, 128, 128, 128, 2]\n",
    "\n",
    "#layers = [3, 100, 100, 100, 100, 2]\n",
    "\n",
    "# Load Data\n",
    "import os\n",
    "data_path = os.path.abspath(\"turing.npy\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = np.load(f)\n",
    "    \n",
    "data_path = os.path.abspath(\"turing_t.npy\")\n",
    "with open(data_path, 'rb') as f:\n",
    "    t_star = np.load(f) \n",
    "    \n",
    "T = t_star.shape[0]    \n",
    "    \n",
    "L = 50\n",
    "x_size = data.shape[1]\n",
    "y_size = data.shape[2]\n",
    "N = x_size*y_size\n",
    "x_domain = L*np.linspace(0,1,x_size)\n",
    "y_domain = L*np.linspace(0,1,y_size)\n",
    "\n",
    "X,Y = np.meshgrid(x_domain, y_domain, sparse=False, indexing='ij')\n",
    "XX = np.tile(X.flatten(), T) # N x T\n",
    "YY = np.tile(Y.flatten(), T) # N x T\n",
    "TT = np.repeat(t_star[-T:], N) # T x N\n",
    "\n",
    "AA = np.einsum('ijk->kij', data[0, :, :, -T:]).flatten() # N x T\n",
    "SS = np.einsum('ijk->kij', data[1, :, :, -T:]).flatten() # N x T\n",
    "\n",
    "\n",
    "x = XX[:, np.newaxis] # NT x 1\n",
    "y = YY[:, np.newaxis] # NT x 1\n",
    "t = TT[:, np.newaxis] # NT x 1\n",
    "\n",
    "a = AA[:, np.newaxis] # NT x 1\n",
    "s = SS[:, np.newaxis] # NT x 1\n",
    "\n",
    "boundary_x_LB = np.concatenate((x_domain, \n",
    "                                np.repeat(x_domain[0], y_size)))\n",
    "boundary_x_TR = np.concatenate((x_domain, \n",
    "                                np.repeat(x_domain[-1], y_size))) \n",
    "\n",
    "boundary_y_LB = np.concatenate((np.repeat(y_domain[0], x_size),\n",
    "                                y_domain))\n",
    "boundary_y_TR = np.concatenate((np.repeat(y_domain[-1], x_size),\n",
    "                                y_domain)) \n",
    "\n",
    "boundary_XX_LB = np.tile(boundary_x_LB.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_XX_TR = np.tile(boundary_x_TR.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_YY_LB = np.tile(boundary_y_LB.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_YY_TR = np.tile(boundary_y_TR.flatten(), T)[:, np.newaxis] # (x_size + y_size) x T, 1\n",
    "boundary_TT = np.repeat(t_star[-T:], (x_size + y_size))[:, np.newaxis] # T x (x_size + y_size), 1\n",
    "\n",
    "\n",
    "def create_dataset(training_data_size =  T*16,\n",
    "                   pde_data_size =  (T*N)//(32),\n",
    "                   boundary_data_size = ((x_size + y_size)*T)//(8),\n",
    "                   with_boundary = True,\n",
    "                   signal_to_noise = 0):\n",
    "    \n",
    "    ##########################################\n",
    "    # Including noise\n",
    "    if signal_to_noise > 0:\n",
    "        signal_amp_a = (np.max(AA)-np.min(AA))/2.0\n",
    "        signal_amp_s = (np.max(SS)-np.min(SS))/2.0  \n",
    "        sigma_a =  signal_amp_a*signal_to_noise\n",
    "        sigma_s =  signal_amp_s*signal_to_noise\n",
    "    # Observed data\n",
    "    idx_data = np.random.choice(N*T, training_data_size, replace=False)\n",
    "    # PDE colocations\n",
    "    idx_pde = np.random.choice(N*T, pde_data_size, replace=False)\n",
    "    # Periodic boundary condition\n",
    "    idx_boundary = np.random.choice((x_size + y_size)*T, boundary_data_size, replace=False)\n",
    "    \n",
    "    ret = {'obs_input': np.c_[XX[idx_data], YY[idx_data],TT[idx_data]],\n",
    "           'obs_output': np.c_[AA[idx_data], SS[idx_data]],\n",
    "           'pde':   np.c_[XX[idx_pde], YY[idx_pde], TT[idx_pde]]}\n",
    "    \n",
    "    if signal_to_noise > 0:        \n",
    "        ret['obs_output'][0] += sigma_a * np.random.randn(len(idx_data), a.shape[1])\n",
    "        ret['obs_output'][1] += sigma_s * np.random.randn(len(idx_data), s.shape[1])\n",
    "    \n",
    "    if with_boundary:\n",
    "        ret = {**ret,\n",
    "               **{'boundary_LB': np.c_[boundary_XX_LB[idx_boundary], \n",
    "                                       boundary_YY_LB[idx_boundary],\n",
    "                                       boundary_TT[idx_boundary]],\n",
    "                  'boundary_TR': np.c_[boundary_XX_TR[idx_boundary],\n",
    "                                       boundary_YY_TR[idx_boundary],\n",
    "                                       boundary_TT[idx_boundary]]\n",
    "                 }\n",
    "              }\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "0104acc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:43:39.957038Z",
     "start_time": "2022-06-14T14:43:39.620499Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params_1 = {'training_data_size': T*16,\n",
    "                'pde_data_size': (T*N)//(32),\n",
    "                'boundary_data_size':((x_size + y_size)*T)//(8)}\n",
    "\n",
    "dataset = create_dataset(**model_params_1)\n",
    "lb, ub = lower_upper_bounds([np.c_[XX, YY, TT]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "532ddbc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:44:33.660289Z",
     "start_time": "2022-06-14T14:44:33.651547Z"
    }
   },
   "outputs": [],
   "source": [
    "pinn = NN(layers, lb, ub)\n",
    "losses = [Observations(pinn,\n",
    "                       inputs_obs = dataset['obs_input'],\n",
    "                       outputs_obs = dataset['obs_output']),\n",
    "         ASDM(pinn,\n",
    "              inputs_pde = dataset['pde']),\n",
    "         Periodic_boundary(pinn,\n",
    "                          inputs_LB_boundary = dataset['boundary_LB'], \n",
    "                          inputs_TR_boundary = dataset['boundary_TR'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "03a553f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T14:44:34.403612Z",
     "start_time": "2022-06-14T14:44:34.204349Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TINN(pinn, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "44bfa049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T15:26:38.688297Z",
     "start_time": "2022-06-14T14:44:37.713750Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss:35366.68\n",
      "Loss_observations:20916.99372356 \n",
      "Loss_ASDM:14315.33350189 sigma_a:0:0.00603408, sigma_s:0:0.99212040, mu_a:0:0.99302148, rho_a:0:1.00040734, rho_s:0:1.00658745, kappa_a:0:0.99774838\n",
      "Loss_Periodic_Boundary:134.35105874 \n",
      "Time:7.91\n",
      "\n",
      "Epoch: 200, loss:2154.15\n",
      "Loss_observations:2148.29761518 \n",
      "Loss_ASDM:3.13478911 sigma_a:0:0.09563690, sigma_s:0:0.87381018, mu_a:0:0.88763274, rho_a:0:1.10939141, rho_s:0:1.22832552, kappa_a:0:0.78303254\n",
      "Loss_Periodic_Boundary:2.72096386 \n",
      "Time:49.33\n",
      "\n",
      "Epoch: 400, loss:2153.44\n",
      "Loss_observations:2147.57618775 \n",
      "Loss_ASDM:2.85946714 sigma_a:0:0.09705297, sigma_s:0:0.86839066, mu_a:0:0.88323627, rho_a:0:1.10658371, rho_s:0:1.22672553, kappa_a:0:0.79145755\n",
      "Loss_Periodic_Boundary:3.00134198 \n",
      "Time:49.25\n",
      "\n",
      "Epoch: 600, loss:2152.08\n",
      "Loss_observations:2147.87941326 \n",
      "Loss_ASDM:2.22758473 sigma_a:0:0.10014777, sigma_s:0:0.85579315, mu_a:0:0.87587571, rho_a:0:1.09978354, rho_s:0:1.22232146, kappa_a:0:0.80907126\n",
      "Loss_Periodic_Boundary:1.97274080 \n",
      "Time:49.86\n",
      "\n",
      "Epoch: 800, loss:2151.13\n",
      "Loss_observations:2148.25967366 \n",
      "Loss_ASDM:1.05702872 sigma_a:0:0.10163753, sigma_s:0:0.84391383, mu_a:0:0.86491193, rho_a:0:1.09554565, rho_s:0:1.21549067, kappa_a:0:0.82429461\n",
      "Loss_Periodic_Boundary:1.81071152 \n",
      "Time:49.63\n",
      "\n",
      "Epoch: 1000, loss:2150.07\n",
      "Loss_observations:2147.31977189 \n",
      "Loss_ASDM:0.98904553 sigma_a:0:0.10142326, sigma_s:0:0.83211778, mu_a:0:0.85504849, rho_a:0:1.09079227, rho_s:0:1.20714955, kappa_a:0:0.84045482\n",
      "Loss_Periodic_Boundary:1.75867583 \n",
      "Time:48.58\n",
      "\n",
      "Epoch: 1200, loss:2152.10\n",
      "Loss_observations:2147.88367492 \n",
      "Loss_ASDM:2.32629580 sigma_a:0:0.09748983, sigma_s:0:0.82058259, mu_a:0:0.84609216, rho_a:0:1.08542030, rho_s:0:1.20045344, kappa_a:0:0.85389499\n",
      "Loss_Periodic_Boundary:1.89499654 \n",
      "Time:49.00\n",
      "\n",
      "Epoch: 1400, loss:2149.28\n",
      "Loss_observations:2146.60452699 \n",
      "Loss_ASDM:0.77934774 sigma_a:0:0.09488735, sigma_s:0:0.81106574, mu_a:0:0.83356234, rho_a:0:1.08400440, rho_s:0:1.19266399, kappa_a:0:0.86549958\n",
      "Loss_Periodic_Boundary:1.89212765 \n",
      "Time:48.39\n",
      "\n",
      "Epoch: 1600, loss:2149.88\n",
      "Loss_observations:2147.37785290 \n",
      "Loss_ASDM:0.84414288 sigma_a:0:0.08838207, sigma_s:0:0.80144569, mu_a:0:0.82307252, rho_a:0:1.08082741, rho_s:0:1.18620358, kappa_a:0:0.87593268\n",
      "Loss_Periodic_Boundary:1.65558684 \n",
      "Time:48.48\n",
      "\n",
      "Epoch: 1800, loss:2149.79\n",
      "Loss_observations:2147.00978061 \n",
      "Loss_ASDM:1.32883884 sigma_a:0:0.08367724, sigma_s:0:0.79262428, mu_a:0:0.80982438, rho_a:0:1.07997404, rho_s:0:1.17721170, kappa_a:0:0.88742844\n",
      "Loss_Periodic_Boundary:1.44941385 \n",
      "Time:48.33\n",
      "\n",
      "Epoch: 2000, loss:2149.63\n",
      "Loss_observations:2145.46977129 \n",
      "Loss_ASDM:1.46735837 sigma_a:0:0.07507717, sigma_s:0:0.78019093, mu_a:0:0.79919798, rho_a:0:1.07551864, rho_s:0:1.17026313, kappa_a:0:0.89979196\n",
      "Loss_Periodic_Boundary:2.69727091 \n",
      "Time:48.48\n",
      "\n",
      "Epoch: 2200, loss:2147.33\n",
      "Loss_observations:2143.78048116 \n",
      "Loss_ASDM:1.02975393 sigma_a:0:0.06907556, sigma_s:0:0.76969341, mu_a:0:0.78395241, rho_a:0:1.07369598, rho_s:0:1.16048182, kappa_a:0:0.91179053\n",
      "Loss_Periodic_Boundary:2.51921643 \n",
      "Time:48.67\n",
      "\n",
      "Epoch: 2400, loss:2146.80\n",
      "Loss_observations:2138.66845743 \n",
      "Loss_ASDM:2.61070734 sigma_a:0:0.05846283, sigma_s:0:0.75693258, mu_a:0:0.76250658, rho_a:0:1.07048654, rho_s:0:1.14841601, kappa_a:0:0.92516159\n",
      "Loss_Periodic_Boundary:5.52333255 \n",
      "Time:48.78\n",
      "\n",
      "Epoch: 2600, loss:2143.95\n",
      "Loss_observations:2135.44261888 \n",
      "Loss_ASDM:2.41591174 sigma_a:0:0.04461285, sigma_s:0:0.74029259, mu_a:0:0.73947751, rho_a:0:1.06730564, rho_s:0:1.13443217, kappa_a:0:0.94326056\n",
      "Loss_Periodic_Boundary:6.08840640 \n",
      "Time:49.83\n",
      "\n",
      "Epoch: 2800, loss:2138.06\n",
      "Loss_observations:2125.81697823 \n",
      "Loss_ASDM:2.73502510 sigma_a:0:0.02009421, sigma_s:0:0.71659453, mu_a:0:0.70873588, rho_a:0:1.06458741, rho_s:0:1.11310066, kappa_a:0:0.97115718\n",
      "Loss_Periodic_Boundary:9.50829318 \n",
      "Time:48.80\n",
      "\n",
      "Epoch: 3000, loss:2083.89\n",
      "Loss_observations:2053.50863809 \n",
      "Loss_ASDM:12.16029418 sigma_a:0:0.00194399, sigma_s:0:0.64498437, mu_a:0:0.64157517, rho_a:0:1.04235446, rho_s:0:1.04794350, kappa_a:0:1.05754622\n",
      "Loss_Periodic_Boundary:18.22195130 \n",
      "Time:48.86\n",
      "\n",
      "Epoch: 3200, loss:1901.65\n",
      "Loss_observations:1824.33638724 \n",
      "Loss_ASDM:35.81187156 sigma_a:0:0.00000000, sigma_s:0:0.48141007, mu_a:0:0.46941924, rho_a:0:0.84424403, rho_s:0:0.86067609, kappa_a:0:1.24893358\n",
      "Loss_Periodic_Boundary:41.50468871 \n",
      "Time:48.53\n",
      "\n",
      "Epoch: 3400, loss:1253.72\n",
      "Loss_observations:1104.60258051 \n",
      "Loss_ASDM:105.57287879 sigma_a:0:0.00000000, sigma_s:0:0.14741263, mu_a:0:0.00000000, rho_a:0:0.00910624, rho_s:0:0.30108313, kappa_a:0:1.50538840\n",
      "Loss_Periodic_Boundary:43.54549518 \n",
      "Time:49.51\n",
      "\n",
      "Epoch: 3600, loss:919.27\n",
      "Loss_observations:730.61739728 \n",
      "Loss_ASDM:162.56967142 sigma_a:0:0.00000000, sigma_s:0:0.07772014, mu_a:0:0.00000000, rho_a:0:0.02179736, rho_s:0:0.15703175, kappa_a:0:1.42343401\n",
      "Loss_Periodic_Boundary:26.08105372 \n",
      "Time:50.57\n",
      "\n",
      "Epoch: 3800, loss:809.67\n",
      "Loss_observations:600.61889496 \n",
      "Loss_ASDM:188.85091005 sigma_a:0:0.00000000, sigma_s:0:0.06949839, mu_a:0:0.00000000, rho_a:0:0.02779244, rho_s:0:0.10224196, kappa_a:0:0.78340796\n",
      "Loss_Periodic_Boundary:20.19926636 \n",
      "Time:51.11\n",
      "\n",
      "Epoch: 4000, loss:615.62\n",
      "Loss_observations:454.56907056 \n",
      "Loss_ASDM:137.69937313 sigma_a:0:0.00000000, sigma_s:0:0.05721125, mu_a:0:0.58544331, rho_a:0:0.46535650, rho_s:0:0.04317127, kappa_a:0:0.05393273\n",
      "Loss_Periodic_Boundary:23.34732948 \n",
      "Time:51.08\n",
      "\n",
      "Epoch: 4200, loss:519.35\n",
      "Loss_observations:369.13004182 \n",
      "Loss_ASDM:124.75930632 sigma_a:0:0.00000000, sigma_s:0:0.05917403, mu_a:0:0.79988803, rho_a:0:0.68317631, rho_s:0:0.04893950, kappa_a:0:0.11116442\n",
      "Loss_Periodic_Boundary:25.46396385 \n",
      "Time:51.49\n",
      "\n",
      "Epoch: 4400, loss:432.99\n",
      "Loss_observations:303.61245155 \n",
      "Loss_ASDM:112.94603189 sigma_a:0:0.00000000, sigma_s:0:0.05985222, mu_a:0:0.95436823, rho_a:0:0.84960069, rho_s:0:0.05201517, kappa_a:0:0.14678545\n",
      "Loss_Periodic_Boundary:16.42666510 \n",
      "Time:52.75\n",
      "\n",
      "Epoch: 4600, loss:394.85\n",
      "Loss_observations:265.06282903 \n",
      "Loss_ASDM:107.56397280 sigma_a:0:0.00000000, sigma_s:0:0.06087532, mu_a:0:1.06721568, rho_a:0:0.97272370, rho_s:0:0.05359424, kappa_a:0:0.16682289\n",
      "Loss_Periodic_Boundary:22.22649445 \n",
      "Time:51.14\n",
      "\n",
      "Epoch: 4800, loss:353.70\n",
      "Loss_observations:231.83463262 \n",
      "Loss_ASDM:104.46878783 sigma_a:0:0.00066257, sigma_s:0:0.06099861, mu_a:0:1.15002590, rho_a:0:1.06152241, rho_s:0:0.05519835, kappa_a:0:0.17830106\n",
      "Loss_Periodic_Boundary:17.39888329 \n",
      "Time:50.52\n",
      "\n",
      "Epoch: 5000, loss:340.61\n",
      "Loss_observations:210.27054496 \n",
      "Loss_ASDM:102.00563161 sigma_a:0:0.00781000, sigma_s:0:0.06102835, mu_a:0:1.21786279, rho_a:0:1.12291344, rho_s:0:0.05503872, kappa_a:0:0.18274001\n",
      "Loss_Periodic_Boundary:28.33696147 \n",
      "Time:50.71\n",
      "\n",
      "Epoch: 5200, loss:303.47\n",
      "Loss_observations:188.91924506 \n",
      "Loss_ASDM:98.24516604 sigma_a:0:0.01703719, sigma_s:0:0.06066144, mu_a:0:1.27903579, rho_a:0:1.17154176, rho_s:0:0.05590372, kappa_a:0:0.18318499\n",
      "Loss_Periodic_Boundary:16.30524039 \n",
      "Time:51.50\n",
      "\n",
      "Epoch: 5400, loss:280.87\n",
      "Loss_observations:171.82016438 \n",
      "Loss_ASDM:96.02688565 sigma_a:0:0.02691062, sigma_s:0:0.06082410, mu_a:0:1.33419120, rho_a:0:1.21032245, rho_s:0:0.05564232, kappa_a:0:0.18197972\n",
      "Loss_Periodic_Boundary:13.02101723 \n",
      "Time:51.12\n",
      "\n",
      "Epoch: 5600, loss:267.15\n",
      "Loss_observations:159.25579328 \n",
      "Loss_ASDM:93.24289837 sigma_a:0:0.03702815, sigma_s:0:0.06031628, mu_a:0:1.38385606, rho_a:0:1.24480458, rho_s:0:0.05622626, kappa_a:0:0.18050378\n",
      "Loss_Periodic_Boundary:14.65187857 \n",
      "Time:50.22\n",
      "\n",
      "Epoch: 5800, loss:263.72\n",
      "Loss_observations:152.34554008 \n",
      "Loss_ASDM:93.10343985 sigma_a:0:0.04667659, sigma_s:0:0.06060782, mu_a:0:1.42898098, rho_a:0:1.27430475, rho_s:0:0.05584082, kappa_a:0:0.17875284\n",
      "Loss_Periodic_Boundary:18.26674518 \n",
      "Time:51.14\n",
      "\n",
      "Epoch: 6000, loss:238.75\n",
      "Loss_observations:138.25124915 \n",
      "Loss_ASDM:87.99466313 sigma_a:0:0.05540723, sigma_s:0:0.06070902, mu_a:0:1.47128702, rho_a:0:1.30124956, rho_s:0:0.05568294, kappa_a:0:0.17648382\n",
      "Loss_Periodic_Boundary:12.50904659 \n",
      "Time:51.95\n",
      "\n",
      "Epoch: 6200, loss:231.44\n",
      "Loss_observations:132.53063678 \n",
      "Loss_ASDM:86.92570500 sigma_a:0:0.06473553, sigma_s:0:0.06048452, mu_a:0:1.51135209, rho_a:0:1.32644792, rho_s:0:0.05591325, kappa_a:0:0.17462648\n",
      "Loss_Periodic_Boundary:11.98625832 \n",
      "Time:51.02\n",
      "\n",
      "Epoch: 6400, loss:224.66\n",
      "Loss_observations:126.92078993 \n",
      "Loss_ASDM:87.71753799 sigma_a:0:0.07256584, sigma_s:0:0.06106220, mu_a:0:1.54856342, rho_a:0:1.35034953, rho_s:0:0.05526451, kappa_a:0:0.17393045\n",
      "Loss_Periodic_Boundary:10.01899368 \n",
      "Time:50.87\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6600, loss:209.17\n",
      "Loss_observations:118.57374295 \n",
      "Loss_ASDM:82.33954680 sigma_a:0:0.08059113, sigma_s:0:0.06089887, mu_a:0:1.58343312, rho_a:0:1.37485510, rho_s:0:0.05551209, kappa_a:0:0.17264844\n",
      "Loss_Periodic_Boundary:8.25979331 \n",
      "Time:50.60\n",
      "\n",
      "Epoch: 6800, loss:217.60\n",
      "Loss_observations:120.26421312 \n",
      "Loss_ASDM:84.91817401 sigma_a:0:0.08722540, sigma_s:0:0.06108271, mu_a:0:1.61744252, rho_a:0:1.39792615, rho_s:0:0.05524146, kappa_a:0:0.17264012\n",
      "Loss_Periodic_Boundary:12.42253154 \n",
      "Time:50.26\n",
      "\n",
      "Epoch: 7000, loss:205.32\n",
      "Loss_observations:112.39236218 \n",
      "Loss_ASDM:81.53608923 sigma_a:0:0.09420186, sigma_s:0:0.06168265, mu_a:0:1.65046280, rho_a:0:1.42142678, rho_s:0:0.05503300, kappa_a:0:0.17218752\n",
      "Loss_Periodic_Boundary:11.39639236 \n",
      "Time:49.62\n",
      "\n",
      "Epoch: 7200, loss:203.64\n",
      "Loss_observations:107.85555878 \n",
      "Loss_ASDM:80.56930813 sigma_a:0:0.10061383, sigma_s:0:0.06158544, mu_a:0:1.68177081, rho_a:0:1.44475128, rho_s:0:0.05527783, kappa_a:0:0.17241134\n",
      "Loss_Periodic_Boundary:15.21767334 \n",
      "Time:50.10\n",
      "\n",
      "Epoch: 7400, loss:190.44\n",
      "Loss_observations:103.66948387 \n",
      "Loss_ASDM:77.37210871 sigma_a:0:0.10630449, sigma_s:0:0.06138940, mu_a:0:1.71359658, rho_a:0:1.46778405, rho_s:0:0.05578027, kappa_a:0:0.17261237\n",
      "Loss_Periodic_Boundary:9.39927873 \n",
      "Time:49.24\n",
      "\n",
      "Epoch: 7600, loss:184.31\n",
      "Loss_observations:101.18814149 \n",
      "Loss_ASDM:74.34432200 sigma_a:0:0.11262068, sigma_s:0:0.06164484, mu_a:0:1.74429906, rho_a:0:1.49137987, rho_s:0:0.05574273, kappa_a:0:0.17228042\n",
      "Loss_Periodic_Boundary:8.78171944 \n",
      "Time:50.05\n",
      "\n",
      "Epoch: 7800, loss:189.45\n",
      "Loss_observations:101.31411484 \n",
      "Loss_ASDM:74.81146914 sigma_a:0:0.11911335, sigma_s:0:0.06062067, mu_a:0:1.77363358, rho_a:0:1.51387841, rho_s:0:0.05655826, kappa_a:0:0.17294049\n",
      "Loss_Periodic_Boundary:13.32461308 \n",
      "Time:51.12\n",
      "\n",
      "Epoch: 8000, loss:176.62\n",
      "Loss_observations:95.47807402 \n",
      "Loss_ASDM:72.61457141 sigma_a:0:0.12478815, sigma_s:0:0.06143472, mu_a:0:1.80421140, rho_a:0:1.53649136, rho_s:0:0.05564744, kappa_a:0:0.17305683\n",
      "Loss_Periodic_Boundary:8.52553008 \n",
      "Time:50.80\n",
      "\n",
      "Epoch: 8200, loss:175.02\n",
      "Loss_observations:93.48469421 \n",
      "Loss_ASDM:72.38187182 sigma_a:0:0.13046509, sigma_s:0:0.06184901, mu_a:0:1.83241155, rho_a:0:1.55851054, rho_s:0:0.05558301, kappa_a:0:0.17423592\n",
      "Loss_Periodic_Boundary:9.14918104 \n",
      "Time:50.76\n",
      "\n",
      "Epoch: 8400, loss:177.15\n",
      "Loss_observations:92.85451552 \n",
      "Loss_ASDM:73.91799156 sigma_a:0:0.13544550, sigma_s:0:0.06206642, mu_a:0:1.86140892, rho_a:0:1.58014685, rho_s:0:0.05550231, kappa_a:0:0.17505506\n",
      "Loss_Periodic_Boundary:10.37953984 \n",
      "Time:50.06\n",
      "\n",
      "Epoch: 8600, loss:170.59\n",
      "Loss_observations:90.64546834 \n",
      "Loss_ASDM:69.99568492 sigma_a:0:0.14195709, sigma_s:0:0.06181924, mu_a:0:1.88795187, rho_a:0:1.60136159, rho_s:0:0.05581700, kappa_a:0:0.17575399\n",
      "Loss_Periodic_Boundary:9.95141425 \n",
      "Time:51.81\n",
      "\n",
      "Epoch: 8800, loss:164.22\n",
      "Loss_observations:88.31750661 \n",
      "Loss_ASDM:67.64518849 sigma_a:0:0.14744996, sigma_s:0:0.06120726, mu_a:0:1.91408014, rho_a:0:1.62233005, rho_s:0:0.05633258, kappa_a:0:0.17635018\n",
      "Loss_Periodic_Boundary:8.25974191 \n",
      "Time:50.40\n",
      "\n",
      "Epoch: 9000, loss:162.31\n",
      "Loss_observations:87.65870980 \n",
      "Loss_ASDM:66.65931209 sigma_a:0:0.15236354, sigma_s:0:0.06177366, mu_a:0:1.94017623, rho_a:0:1.64271981, rho_s:0:0.05574935, kappa_a:0:0.17695270\n",
      "Loss_Periodic_Boundary:7.99611053 \n",
      "Time:52.35\n",
      "\n",
      "Epoch: 9200, loss:157.88\n",
      "Loss_observations:84.94497141 \n",
      "Loss_ASDM:65.96833991 sigma_a:0:0.15736500, sigma_s:0:0.06112378, mu_a:0:1.96521720, rho_a:0:1.66251113, rho_s:0:0.05636174, kappa_a:0:0.17800583\n",
      "Loss_Periodic_Boundary:6.96733075 \n",
      "Time:51.09\n",
      "\n",
      "Epoch: 9400, loss:169.20\n",
      "Loss_observations:87.72499943 \n",
      "Loss_ASDM:72.39985809 sigma_a:0:0.16245036, sigma_s:0:0.06156308, mu_a:0:1.99027391, rho_a:0:1.68223247, rho_s:0:0.05600855, kappa_a:0:0.17925269\n",
      "Loss_Periodic_Boundary:9.07078169 \n",
      "Time:51.21\n",
      "\n",
      "Epoch: 9600, loss:157.54\n",
      "Loss_observations:84.54228827 \n",
      "Loss_ASDM:64.67200108 sigma_a:0:0.16730761, sigma_s:0:0.06146890, mu_a:0:2.01335413, rho_a:0:1.70154309, rho_s:0:0.05664990, kappa_a:0:0.17979664\n",
      "Loss_Periodic_Boundary:8.32340624 \n",
      "Time:51.00\n",
      "\n",
      "Epoch: 9800, loss:162.35\n",
      "Loss_observations:85.46895171 \n",
      "Loss_ASDM:68.27088921 sigma_a:0:0.17206730, sigma_s:0:0.06155777, mu_a:0:2.03743262, rho_a:0:1.72043194, rho_s:0:0.05584540, kappa_a:0:0.18083651\n",
      "Loss_Periodic_Boundary:8.61510621 \n",
      "Time:51.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr_total_loss, arr_losses = model.train(10000,batch_size=dataset['obs_input'].shape[0]//8, print_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "12685549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T15:29:15.029172Z",
     "start_time": "2022-06-14T15:29:14.686552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arr_losses[:,2])\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c03d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-14T15:41:16.341Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss:155.22\n",
      "Loss_observations:83.18583532 \n",
      "Loss_ASDM:65.03639502 sigma_a:0:0.17624217, sigma_s:0:0.06170394, mu_a:0:2.06095097, rho_a:0:1.73926466, rho_s:0:0.05667740, kappa_a:0:0.18143927\n",
      "Loss_Periodic_Boundary:6.99720099 \n",
      "Time:0.28\n",
      "\n",
      "Epoch: 200, loss:151.77\n",
      "Loss_observations:80.39914812 \n",
      "Loss_ASDM:63.40993794 sigma_a:0:0.18136186, sigma_s:0:0.06272572, mu_a:0:2.08378976, rho_a:0:1.75856722, rho_s:0:0.05564608, kappa_a:0:0.18190478\n",
      "Loss_Periodic_Boundary:7.96396697 \n",
      "Time:51.12\n",
      "\n",
      "Epoch: 400, loss:148.21\n",
      "Loss_observations:80.45783066 \n",
      "Loss_ASDM:60.73595033 sigma_a:0:0.18556268, sigma_s:0:0.06147172, mu_a:0:2.10647888, rho_a:0:1.77705076, rho_s:0:0.05696130, kappa_a:0:0.18250082\n",
      "Loss_Periodic_Boundary:7.01287360 \n",
      "Time:50.19\n",
      "\n",
      "Epoch: 600, loss:146.81\n",
      "Loss_observations:78.56147738 \n",
      "Loss_ASDM:60.81958897 sigma_a:0:0.19008030, sigma_s:0:0.06168259, mu_a:0:2.12968006, rho_a:0:1.79549527, rho_s:0:0.05671472, kappa_a:0:0.18345644\n",
      "Loss_Periodic_Boundary:7.43093504 \n",
      "Time:49.81\n",
      "\n",
      "Epoch: 800, loss:149.74\n",
      "Loss_observations:78.46044771 \n",
      "Loss_ASDM:63.65653337 sigma_a:0:0.19441769, sigma_s:0:0.06225424, mu_a:0:2.15206030, rho_a:0:1.81428920, rho_s:0:0.05666737, kappa_a:0:0.18417063\n",
      "Loss_Periodic_Boundary:7.62701555 \n",
      "Time:49.46\n",
      "\n",
      "Epoch: 1000, loss:141.78\n",
      "Loss_observations:76.90233141 \n",
      "Loss_ASDM:58.40328207 sigma_a:0:0.19844340, sigma_s:0:0.06211690, mu_a:0:2.17460258, rho_a:0:1.83307204, rho_s:0:0.05652427, kappa_a:0:0.18462502\n",
      "Loss_Periodic_Boundary:6.47036573 \n",
      "Time:49.53\n",
      "\n",
      "Epoch: 1200, loss:142.41\n",
      "Loss_observations:76.12815531 \n",
      "Loss_ASDM:59.00206348 sigma_a:0:0.20217663, sigma_s:0:0.06229410, mu_a:0:2.19724064, rho_a:0:1.85111177, rho_s:0:0.05667444, kappa_a:0:0.18561184\n",
      "Loss_Periodic_Boundary:7.27602488 \n",
      "Time:50.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arr_total_loss2, arr_losses2 = model.train(10000,batch_size=dataset['obs_input'].shape[0]//8, print_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127833bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe03b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ea7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c358f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0fe7e4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:29:12.766890Z",
     "start_time": "2022-06-14T13:29:12.758447Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = [3, 10, 10, 10, 2]\n",
    "sample_xyt  = np.array([ [0, 0, 0],\n",
    "                         [0, 1, 0],\n",
    "                         [1, 0, 0],\n",
    "                         [1, 1, 0],\n",
    "                         [0, 0, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [1, 0, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [0, 0, 2],\n",
    "                         [0, 1, 2],\n",
    "                         [1, 0, 2],\n",
    "                         [1, 1, 2]\n",
    "                       ])\n",
    "\n",
    "lb, ub = lower_upper_bounds([sample_xyt])\n",
    "#print(normalise_inputs([sample_xyt, sample_xyt.copy()*2]))\n",
    "\n",
    "model = NN(layers, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e8e9ae9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:29:52.098053Z",
     "start_time": "2022-06-14T13:29:52.090164Z"
    }
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "#disable_eager_execution()\n",
    "i = tf.constant(sample_xyt*1.0)\n",
    "#o, p1, p2 = model(i, True)\n",
    "o = model(i, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8621cf0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T13:30:39.327864Z",
     "start_time": "2022-06-14T13:30:39.322710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_weights == model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "380f887e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T00:01:55.325494Z",
     "start_time": "2022-06-14T00:01:55.319151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs_input', 'obs_output', 'pde', 'boundary_LB', 'boundary_TR'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5486f0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T23:59:32.048168Z",
     "start_time": "2022-06-13T23:59:32.040492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3fcc8810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T00:02:03.037282Z",
     "start_time": "2022-06-14T00:02:03.029234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['boundary_LB'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1280e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39233797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e0cec75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:26:45.104615Z",
     "start_time": "2022-06-13T18:26:45.095968Z"
    }
   },
   "outputs": [],
   "source": [
    "asdm = ASDM(layers, \n",
    "           inputs_obs = np.c_[dataset['x_obs'], dataset['y_obs'], dataset['t_obs']],\n",
    "           output_obs = np.c_[dataset['a_obs'], dataset['s_obs']]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1ce2f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:27:18.260569Z",
     "start_time": "2022-06-13T18:26:50.155339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss:35680.41, \n",
      "Time:1.70\n",
      "\n",
      "Epoch: 100, loss:2178.56, \n",
      "Time:6.64\n",
      "\n",
      "Epoch: 200, loss:2176.05, \n",
      "Time:6.57\n",
      "\n",
      "Epoch: 300, loss:2176.39, \n",
      "Time:6.57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asdm.train(400, T*4, print_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1a3a469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T18:27:19.832997Z",
     "start_time": "2022-06-13T18:27:19.820377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sigma_a:0' shape=(1,) dtype=float64, numpy=array([0.10185468])>,\n",
       " <tf.Variable 'sigma_s:0' shape=(1,) dtype=float64, numpy=array([0.83944022])>,\n",
       " <tf.Variable 'mu_a:0' shape=(1,) dtype=float64, numpy=array([0.88263012])>,\n",
       " <tf.Variable 'rho_a:0' shape=(1,) dtype=float64, numpy=array([1.12305397])>,\n",
       " <tf.Variable 'rho_s:0' shape=(1,) dtype=float64, numpy=array([1.19049248])>,\n",
       " <tf.Variable 'kappa_a:0' shape=(1,) dtype=float64, numpy=array([0.82351467])>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdm.model_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63755b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43b561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dce8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0f117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b256b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TINN2():\n",
    "    \"\"\"Turing-Informed Neoral Net\"\"\"\n",
    "    def __init__(self, \n",
    "                 layers, \n",
    "                 inputs_obs,\n",
    "                 output_obs,\n",
    "                 inputs_pde = None,\n",
    "                 boundary_spec = dict(\n",
    "                    inputs_boundary = None,\n",
    "                    boundary_loss_callback = None,\n",
    "                    needs_grad = False \n",
    "                   )                 \n",
    "                 ):\n",
    "        self.model = NN(layers)\n",
    "        self.inputs_obs = inputs_obs\n",
    "        self.output_obs = output_obs        \n",
    "        # Use the observation points for validating\n",
    "        # PDE\n",
    "        if inputs_pde is None:\n",
    "            self.inputs_pde = inputs_obs\n",
    "        else:\n",
    "            self.inputs_pde = inputs_pde\n",
    "            \n",
    "        self.boundary_spec = boundary_spec\n",
    "        if boundary_spec['inputs_boundary'] is None:\n",
    "            self.has_bounday = False\n",
    "        else:\n",
    "            self.has_bounday = True\n",
    "        if self.boundary_spec['boundary_loss_callback'] is None:\n",
    "            self.boundary_loss_callback = self.periodic_boundary\n",
    "        else:\n",
    "            self.boundary_loss_callback = self.boundary_spec['boundary_loss_callback']        \n",
    "        \n",
    "        self.optimizer_Adam = keras.optimizers.Adam()\n",
    "        #self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        #self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "     \n",
    "    def periodic_boundary(self, inputs, boundary_pred, boundary_spec):        \n",
    "        pass\n",
    "    \n",
    "    def pde_residuals(self, pde_inputs, pde_outputs, partials_1, partials_2):\n",
    "        pass\n",
    "    \n",
    "    def model_vars(self):\n",
    "        return []\n",
    "    \n",
    "    def loss_obs(self, inputs, outputs):\n",
    "        obs_pred = self.model(inputs, grads = False)\n",
    "        L = tf.reduce_sum(tf.square(obs_pred - outputs), name = \"Loss_observations\")\n",
    "        return L\n",
    "    \n",
    "    def loss_pde(self, inputs):\n",
    "        pde_outputs, partials_1, partials_2 = self.model(inputs, grads = True)\n",
    "        pde_res = self.pde_residuals(inputs, pde_outputs, partials_1, partials_2)\n",
    "        L = tf.reduce_sum(tf.square(pde_res), name = \"Loss_pde\")\n",
    "        return L\n",
    "        \n",
    "    def loss_boundary(self, inputs, outputs):\n",
    "        boundary_pred = self.model(inputs, self.boundary_spec['needs_grad'])        \n",
    "        L = self.boundary_loss_callback(inputs, boundary_pred, self.boundary_spec)        \n",
    "        return L\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __batches__(self, batch_size):\n",
    "        \"\"\"Generator that returned shuffled indeces for each batch\"\"\"\n",
    "        \n",
    "        flg_boundary = self.has_bounday\n",
    "        # Observation batch info \n",
    "        obs_n = self.inputs_obs.shape[0]\n",
    "        batch_steps = obs_n//batch_size\n",
    "        batch_steps = batch_steps + (obs_n-1)//(batch_steps*batch_size)\n",
    "        # PDE batch info\n",
    "        pde_n = self.inputs_pde.shape[0]\n",
    "        pde_batch_size = pde_n//batch_steps        \n",
    "        # Boundary condition batch info\n",
    "        if flg_boundary:\n",
    "            boundary_n = self.boundary_spec.inputs_boundary.shape[0]        \n",
    "            boundary_batch_size = boundary_n//batch_steps\n",
    "        # Observation indices   \n",
    "        indices_obs = np.array(list(range(obs_n)))\n",
    "        np.random.shuffle(indices_obs)\n",
    "        # PDE  indices\n",
    "        indices_pde = np.array(list(range(pde_n)))        \n",
    "        np.random.shuffle(indices_pde)\n",
    "        # Boundary condition  indices\n",
    "        if flg_boundary:\n",
    "            indices_boundary = np.array(list(range(boundary_n)))\n",
    "            np.random.shuffle(indices_boundary)\n",
    "            \n",
    "        for batch in range(batch_steps):\n",
    "            # Observation start-end\n",
    "            obs_start = batch*batch_size\n",
    "            obs_end = (batch+1)*batch_size\n",
    "            obs_end = obs_end - (obs_end//obs_n)*(obs_end%obs_n)\n",
    "            # PDE  start-end\n",
    "            pde_start = batch*pde_batch_size\n",
    "            pde_end = (batch+1)*pde_batch_size            \n",
    "            # Boundary condition  start-end\n",
    "            if flg_boundary:\n",
    "                boundary_start = batch*boundary_batch_size\n",
    "                boundary_end = (batch+1)*boundary_batch_size\n",
    "                \n",
    "            # Correction for PDE and boundary batches at last step\n",
    "            if batch == batch_steps-1:\n",
    "                if pde_end < pde_n:\n",
    "                    pde_end = pde_n\n",
    "                        \n",
    "                if flg_boundary and boundary_end < boundary_n:\n",
    "                    boundary_end = boundary_n\n",
    "            # step's indices        \n",
    "            batch_indices_obs = indices_obs[obs_start:obs_end]\n",
    "            batch_indices_pde = indices_pde[pde_start:pde_end]\n",
    "            if flg_boundary:\n",
    "                batch_indices_boundary = indices_boundary[boundary_start:boundary_end]\n",
    "                yield (batch_indices_obs, batch_indices_pde, batch_indices_boundary)\n",
    "            else:\n",
    "                yield (batch_indices_obs, batch_indices_pde, None)\n",
    "        \n",
    "    def train(self, epochs, batch_size, print_iter=10):\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0            \n",
    "            for obs_indeces, pde_indeces, boundary_indices in self.__batches__(batch_size):                \n",
    "                obs_inputs_batch  = self.inputs_obs[obs_indeces]\n",
    "                obs_outputs_batch = self.output_obs[obs_indeces]\n",
    "                pde_inputs_batch  = self.inputs_pde[pde_indeces] \n",
    "                if self.has_bounday:\n",
    "                    boundary_inputs_batch = self.boundary_spec.inputs_boundary[boundary_indices]\n",
    "                    L = self.train_step(obs_inputs_batch, \n",
    "                                        obs_outputs_batch, \n",
    "                                        pde_inputs_batch,\n",
    "                                        boundary_inputs_batch)\n",
    "                else:\n",
    "                    L = self.train_step(obs_inputs_batch, \n",
    "                                        obs_outputs_batch, \n",
    "                                        pde_inputs_batch,\n",
    "                                        None)\n",
    "                total_loss += L\n",
    "                \n",
    "            if epoch % print_iter == 0:\n",
    "                elapsed = time.time() - start_time                                                                \n",
    "                print(f\"Epoch: {epoch}, loss:{total_loss:.2f}, \\n\"\n",
    "                      f\"Time:{elapsed:.2f}\\n\")\n",
    "                start_time = time.time()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, \n",
    "                   obs_inputs_batch, \n",
    "                   obs_outputs_batch, \n",
    "                   pde_inputs_batch,\n",
    "                   boundary_inputs_batch):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_obs = self.loss_obs(obs_inputs_batch, obs_outputs_batch)\n",
    "            loss_pde = self.loss_pde(pde_inputs_batch)\n",
    "                    \n",
    "            loss = loss_obs + loss_pde\n",
    "            if self.has_bounday:\n",
    "                loss += self.loss_boundary(boundary_inputs_batch)\n",
    "                \n",
    "        \n",
    "        trainable_vars = self.model.trainable_weights + self.model_vars()\n",
    "        grads = tape.gradient(loss,  trainable_vars)\n",
    "        self.optimizer_Adam.apply_gradients(zip(grads, trainable_vars))        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "class ASDM2(TINN):\n",
    "    def __init__(self,\n",
    "                 *arg,\n",
    "                 **kwargs):\n",
    "        super().__init__(*arg, **kwargs)\n",
    "        \n",
    "        \n",
    "        self.sigma_a = tf.Variable([0.0], dtype=tf.float64,\n",
    "                                   name=\"sigma_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.sigma_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                   name=\"sigma_s\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.mu_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                name=\"mu_a\",\n",
    "                                constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_a = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_a\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.rho_s = tf.Variable([1.00], dtype=tf.float64, \n",
    "                                 name=\"rho_s\",\n",
    "                                 constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        self.kappa_a = tf.Variable([1.00], dtype=tf.float64,\n",
    "                                   name=\"kappa_a\",\n",
    "                                   constraint= lambda z: tf.clip_by_value(z, 0, 1e10))\n",
    "        \n",
    "    def model_vars(self):\n",
    "        return [self.sigma_a,\n",
    "                self.sigma_s,\n",
    "                self.mu_a,\n",
    "                self.rho_a,\n",
    "                self.rho_s,\n",
    "                self.kappa_a\n",
    "               ]\n",
    "    \n",
    "    def pde_residuals(self, pde_inputs, pde_outputs, partials_1, partials_2):    \n",
    "        \n",
    "        a = pde_outputs[:, 0]\n",
    "        s = pde_outputs[:, 1]\n",
    "        \n",
    "        a_x = partials_1[0][:, 0]\n",
    "        a_y = partials_1[0][:, 1]\n",
    "        a_t = partials_1[0][:, 2]\n",
    "        \n",
    "        a_xx = partials_2[0][:, 0]\n",
    "        a_yy = partials_2[0][:, 1]\n",
    "        \n",
    "        \n",
    "        s_x = partials_1[1][:, 0]\n",
    "        s_y = partials_1[1][:, 1]\n",
    "        s_t = partials_1[1][:, 2]\n",
    "        \n",
    "        s_xx = partials_2[1][:, 0]\n",
    "        s_yy = partials_2[1][:, 1]\n",
    "        \n",
    "        sigma_a = self.sigma_a\n",
    "        sigma_s = self.sigma_s\n",
    "        mu_a = self.mu_a\n",
    "        rho_a = self.rho_a\n",
    "        rho_s = self.rho_s\n",
    "        kappa_a = self.kappa_a\n",
    "        \n",
    "        one_1 = tf.constant(1.0, dtype=tf.float64)\n",
    "        f = a*a*s/(one_1 + kappa_a*a*a)\n",
    "        f_a = a_t - (a_xx + a_yy) - rho_a*f + mu_a*a - sigma_a\n",
    "        f_s = s_t - (s_xx + s_yy) + rho_s*f - sigma_s\n",
    "        \n",
    "        return tf.concat([tf.expand_dims(f_a,axis=1), \n",
    "                          tf.expand_dims(f_s,axis=1),], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
